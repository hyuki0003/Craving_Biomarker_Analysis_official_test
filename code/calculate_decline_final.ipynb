{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f89794",
   "metadata": {},
   "source": [
    "# 1. 실험 준비\n",
    "\n",
    "## 1-1. 라이브러리 선언"
   ]
  },
  {
   "cell_type": "code",
   "id": "d23bf913",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:55:57.693571Z",
     "start_time": "2025-09-30T09:55:57.028870Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import neurokit2 as nk\n",
    "import re\n",
    "import glob\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from scipy.signal import lombscargle\n",
    "from neurokit2.misc import NeuroKitWarning\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "0e37a7ae",
   "metadata": {},
   "source": "## 1-2. Subject 및 전역 변수 선언"
  },
  {
   "cell_type": "code",
   "id": "8c48fb0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:55:57.726374Z",
     "start_time": "2025-09-30T09:55:57.722163Z"
    }
   },
   "source": [
    "subject_list =[\n",
    " '1_1_011_V2',\n",
    " '1_1_015_V2',\n",
    " '1_1_025_V1',\n",
    " '1_1_027_V1',\n",
    " '1_1_028_V1',\n",
    " '1_1_029_V1',\n",
    " '1_1_034_V1',\n",
    " '1_1_035_V1',\n",
    " '1_1_036_V1',\n",
    " ]\n",
    "# 예시 Subject\n",
    "subject = '1_1_011_V2'\n",
    "\n",
    "DATA_FOLDER = r'..\\features\\joined\\\\'\n",
    "OUTPUT_FOLDER = r'..\\features\\results\\\\'\n",
    "label_path = r'..\\features\\label'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "5b9eca00",
   "metadata": {},
   "source": "## 1-3. ECG, PPG의 Sampling rate 선언"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:55:58.460269Z",
     "start_time": "2025-09-30T09:55:58.456424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "FS_ECG = 512\n",
    "FS_PPG = 51.2"
   ],
   "id": "1566474bb82aee9d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Raw 데이터 준비\n",
    "## 2-1. 데이터 분할 (Data Split)\n",
    "\n",
    "---\n",
    "\n",
    "### Function data_split\n",
    "\n",
    "##### - 주어진 한 Subject의 원본 데이터를 분석하여 VR 타임스탬프를 기준으로 ECG, PPG/GSR 데이터를 'low', 'mid', 'high' 등의 구간으로 분할하고 (output_dir)에 저장.\n",
    "\n",
    "- Args:\n",
    "    - base_dir (str): 'ECG_PPG_GSR', 'VR_Timestamp' 폴더를 포함하는 한 Subject 원본 데이터의 최상위 경로.\n",
    "    - output_dir (str): 분할된 CSV 파일들을 저장할 목적지 경로.\n",
    "\n",
    "\n",
    "### Function load_csv(file_path):\n",
    "##### - Shimmer 센서 데이터 형식의 CSV 파일을 로드.\n",
    "\n",
    "- Args:\n",
    "    - file_path (str): 로드할 CSV 파일의 경로.\n",
    "- Returns:\n",
    "    - pd.DataFrame: 로드된 데이터가 담긴 DataFrame.\n",
    "\n",
    "\n",
    "### Function get_VR_timestamp(file_path):\n",
    "##### - VR 타임스탬프 Excel 파일을 분석하여 각 영상 구간의 시작 및 종료 시각을 추출.\n",
    "\n",
    "- Args:\n",
    "    - file_path (str): VR 타임스탬프 정보가 담긴 Excel 파일의 경로.\n",
    "\n",
    "- Returns:\n",
    "    - list[tuple], list[tuple], list[tuple], list[tuple], list[tuple] (e.g., [('YYYY-MM-DD HH:MM:SS.sss', 'YYYY-MM-DD HH:MM:SS.sss', '...')]):\n",
    "        - 각각 start, end, low, mid, high 구간에 대한 (시작 시각, 종료 시각) 튜플의 리스트.\n",
    "\n",
    "### Function convert_to_unix(time_str):\n",
    "##### - 'YYYY-MM-DD HH:MM:SS.sss' 형식의 시간 문자열을 UNIX 타임스탬프로 변환.\n",
    "- Args:\n",
    "    - time_str (str): 변환할 시간 문자열.\n",
    "\n",
    "- Returns:\n",
    "    - int | None: 변환된 UNIX 타임스탬프 또는 변환 실패 시 None.\n",
    "\n",
    "### Function filter_data_by_time(df, timestamps, timestamp_column_name = 'Timestamp'):\n",
    "\n",
    "##### - 주어진 (시작, 종료) 시각 리스트에 따라 DataFrame을 필터링.\n",
    "\n",
    "- Args:\n",
    "    - df (pd.DataFrame): 필터링할 원본 DataFrame.\n",
    "    - timestamps (list[tuple[str, str]]): (시작 시각, 종료 시각) 문자열 튜플의 리스트.\n",
    "    - timestamp_column_name (str): df에 저장되어있는 UNIX 타임스탬프의 컬럼 명.\n",
    "\n",
    "- Returns:\n",
    "    - list[pd.DataFrame]: 각 타임스탬프 구간에 맞게 필터링된 DataFrame 리스트.\n",
    "\n",
    "### Function save_filtered_data(filtered_data, base_dir, category, prefix):\n",
    "\n",
    "##### - 필터링된 데이터프레임 리스트를 지정된 경로에 CSV 파일로 저장.\n",
    "\n",
    "- Args:\n",
    "    - filtered_data (list[pd.DataFrame]): 저장할 DataFrame의 리스트.\n",
    "    - base_dir (str): 저장할 최상위 경로 (e.g., output_dir).\n",
    "    - category (str): 하위 폴더 이름 및 파일명 (e.g., 'low', 'mid').\n",
    "    - prefix (str): 추출한 데이터의 이름 (e.g., 'PPG', 'ECG').\n"
   ],
   "id": "8b23558ca6f4db6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:55:59.201473Z",
     "start_time": "2025-09-30T09:55:59.179938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_split(base_dir: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    주어진 한 Subject의 원본 데이터를 분석하여 VR 타임스탬프를 기준으로 ECG, PPG/GSR 데이터를 'low', 'mid', 'high' 등의 구간으로 분할하고 (output_dir)에 저장.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): 'ECG_PPG_GSR', 'VR_Timestamp' 폴더를 포함하는 한 Subject 원본 데이터의 최상위 경로.\n",
    "        output_dir (str): 분할된 CSV 파일들을 저장할 목적지 경로.\n",
    "    \"\"\"\n",
    "    # ------------------------- 1. 데이터 경로 변수 초기화 -------------------------\n",
    "    # PPG, GSR 파일 경로\n",
    "    PPG_path = ''\n",
    "    # ECG 파일 경로\n",
    "    ECG_path = ''\n",
    "    # VR Timestamp 파일 경로\n",
    "    VR_timestamp_path = ''\n",
    "\n",
    "    print(base_dir+ ' 데이터 Split 처리 시작')\n",
    "\n",
    "    # ------------------------- 2. Subject 내부를 순회하며 데이터 경로 탐색 -------------------------\n",
    "\n",
    "    # subject 폴더 내부에 ECG_PPG_GSR이라는 폴더가 있는지 확인\n",
    "    if 'ECG_PPG_GSR' in os.listdir(base_dir):\n",
    "        # 'ECG_PPG_GSR' 폴더 내부의 하위 폴더들 순회\n",
    "        for path in os.listdir(base_dir + '/ECG_PPG_GSR'):\n",
    "            temp_path = os.path.join(base_dir, 'ECG_PPG_GSR', path)\n",
    "            ecg_ppg_path = os.listdir(temp_path)[0]\n",
    "            # 파일명에 포함된 장비 ID로 PPG, ECG 경로 특정\n",
    "            if 'id95AE' in ecg_ppg_path:\n",
    "                PPG_path = temp_path + '/' + ecg_ppg_path\n",
    "            elif 'Shimmer_820D' in ecg_ppg_path:\n",
    "                ECG_path = temp_path + '/' + ecg_ppg_path\n",
    "\n",
    "    # subject 폴더 내부 VR_Timestamp 폴더 확인\n",
    "    if 'VR_Timestamp' in os.listdir(base_dir):\n",
    "        VR_timestamp_path = base_dir + '/VR_Timestamp/' + os.listdir(base_dir + '/VR_Timestamp')[0]\n",
    "    # ------------------------- 3. 필수 파일 경로 확인 -------------------------\n",
    "    # 세 개의 경로 중 하나라도 비어있으면 오류 메시지 출력 후 함수 종료\n",
    "    if PPG_path == '':\n",
    "        print('폴더 내 PPG 파일이 없습니다.')\n",
    "        return\n",
    "    elif ECG_path == '':\n",
    "        print('폴더 내 ECG 파일이 없습니다.')\n",
    "        return\n",
    "    elif VR_timestamp_path == '':\n",
    "        print('폴더 내 VR_timestamp 파일이 없습니다.')\n",
    "        return\n",
    "\n",
    "    # 각 센서 데이터의 타임스탬프 컬럼명 정의\n",
    "    PPG_timestamp_column_name = 'id95AE_Timestamp_Unix_CAL'\n",
    "    ECG_timestamp_column_name = 'Shimmer_820D_Timestamp_Unix_CAL'\n",
    "\n",
    "    # ------------------------- 4. 센서 데이터 및 VR 타임스탬프 로드 -------------------------\n",
    "\n",
    "    # 센서 데이터 로드\n",
    "    PPG_df = load_csv(PPG_path)\n",
    "    ECG_df = load_csv(ECG_path)\n",
    "\n",
    "    # VR 타임스탬프 파일에서 각 구간(start, end, low, mid, high)의 시간 정보 추출\n",
    "    start, end, low, mid, high = get_VR_timestamp(file_path=VR_timestamp_path)\n",
    "\n",
    "    # ------------------------- 5. VR 타임스탬프 기준으로 데이터 필터링 (PPG & ECG) -------------------------\n",
    "    # PPG 데이터를 각 구간별로 필터링\n",
    "    start_PPG = filter_data_by_time(PPG_df, start, PPG_timestamp_column_name)\n",
    "    end_PPG = filter_data_by_time(PPG_df, end, PPG_timestamp_column_name)\n",
    "    low_PPG = filter_data_by_time(PPG_df, low, PPG_timestamp_column_name)\n",
    "    mid_PPG = filter_data_by_time(PPG_df, mid, PPG_timestamp_column_name)\n",
    "    high_PPG = filter_data_by_time(PPG_df, high, PPG_timestamp_column_name)\n",
    "\n",
    "    # ECG 데이터를 각 구간별로 필터링\n",
    "    start_ECG = filter_data_by_time(ECG_df, start, ECG_timestamp_column_name)\n",
    "    end_ECG = filter_data_by_time(ECG_df, end, ECG_timestamp_column_name)\n",
    "    low_ECG = filter_data_by_time(ECG_df, low, ECG_timestamp_column_name)\n",
    "    mid_ECG = filter_data_by_time(ECG_df, mid, ECG_timestamp_column_name)\n",
    "    high_ECG = filter_data_by_time(ECG_df, high, ECG_timestamp_column_name)\n",
    "\n",
    "    # ------------------------- 6. 분할된 데이터를 CSV 파일로 저장 -------------------------\n",
    "\n",
    "    # 데이터를 Label에 맞게 저장\n",
    "    save_filtered_data(start_PPG, output_dir, \"start\", \"PPG\")\n",
    "    save_filtered_data(end_PPG, output_dir, \"end\", \"PPG\")\n",
    "    save_filtered_data(start_ECG, output_dir, \"start\", \"ECG\")\n",
    "    save_filtered_data(end_ECG, output_dir, \"end\", \"ECG\")\n",
    "\n",
    "    save_filtered_data(low_PPG, output_dir, \"low\", \"PPG\")\n",
    "    save_filtered_data(mid_PPG, output_dir, \"mid\", \"PPG\")\n",
    "    save_filtered_data(high_PPG, output_dir, \"high\", \"PPG\")\n",
    "\n",
    "    save_filtered_data(low_ECG, output_dir, \"low\", \"ECG\")\n",
    "    save_filtered_data(mid_ECG, output_dir, \"mid\", \"ECG\")\n",
    "    save_filtered_data(high_ECG, output_dir, \"high\", \"ECG\")\n",
    "\n",
    "    print(output_dir+'폴더에 Split데이터를 저장했습니다.\\n')\n",
    "\n",
    "def load_csv(file_path):\n",
    "    \"\"\"\n",
    "    Shimmer 센서 데이터 형식의 CSV 파일을 로드.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): 로드할 CSV 파일의 경로.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 로드된 데이터가 담긴 DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.read_csv(file_path, skiprows=[0, 2], sep='\\t', low_memory=False)\n",
    "\n",
    "def get_VR_timestamp(file_path):\n",
    "    \"\"\"\n",
    "        VR 타임스탬프 Excel 파일을 분석하여 각 영상 구간의 시작 및 종료 시각을 추출.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): VR 타임스탬프 정보가 담긴 Excel 파일의 경로.\n",
    "\n",
    "        Returns:\n",
    "            list[tuple], list[tuple], list[tuple], list[tuple], list[tuple] (e.g., [('YYYY-MM-DD HH:MM:SS.sss', 'YYYY-MM-DD HH:MM:SS.sss', '...')]):\n",
    "                각각 start, end, low, mid, high 구간에 대한 (시작 시각, 종료 시각) 튜플의 리스트.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "\n",
    "    def find_index_by_keyword(column_index, keyword):\n",
    "        \"\"\"\n",
    "            특정 열에서 특정 키워드가 포함된 행의 인덱스를 찾는 함수.\n",
    "\n",
    "            Args:\n",
    "                column_index(int): 검색할 열의 인덱스\n",
    "                keyword(str): 찾을 키워드 (예: '_낮은_', '_중간_', '_높은_')\n",
    "\n",
    "            return:\n",
    "                list | int: 해당 키워드를 포함하는 행의 인덱스\n",
    "        \"\"\"\n",
    "        return df[df.iloc[:, column_index].astype(str).str.contains(keyword, na=False)].index.tolist()\n",
    "\n",
    "    # '괌' 키워드로 영상의 전체 시작과 끝 인덱스 탐색\n",
    "    start_end_range = sorted(find_index_by_keyword(column_index=2, keyword='괌'))\n",
    "    start_range, end_range = start_end_range[0], start_end_range[1]\n",
    "\n",
    "    # 갈망 수준별 영상 인덱스 탐색\n",
    "    low_range = find_index_by_keyword(column_index=2, keyword='_낮은_')\n",
    "    mid_range = find_index_by_keyword(column_index=2, keyword='_중간_')\n",
    "    high_range = find_index_by_keyword(column_index=2, keyword='_높은_')\n",
    "\n",
    "    def extract_timestamps(rng):\n",
    "        \"\"\"\n",
    "            주어진 인덱스 리스트로부터 (시작 시각, 종료 시각) 튜플 리스트를 추출하는 내부 함수.\n",
    "\n",
    "            args:\n",
    "                rng(list): 인덱스 리스트\n",
    "\n",
    "            return:\n",
    "                list: 각 영상의 시작시간과 끝시간을 저장한 리스트\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i in rng:\n",
    "            try:\n",
    "                # 종료 시각(컬럼 9), 재생 시간(컬럼 10) 정보 추출\n",
    "                end_time_raw = df.iloc[i, 9]\n",
    "\n",
    "                # 12시간제(PM/AM 포함)와 24시간제 형식을 구분하여 파싱\n",
    "                if 'PM' in end_time_raw or 'AM' in end_time_raw:\n",
    "                    # 12시간제 처리\n",
    "                    end_time = datetime.strptime(end_time_raw.strip(), \"%Y-%m-%d %I:%M:%S.%f %p\")\n",
    "                else:\n",
    "                    # 24시간제 처리\n",
    "                    end_time_str = end_time_raw[:-3]\n",
    "                    end_time = datetime.strptime(end_time_str, \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "\n",
    "                # 재생시간 파싱\n",
    "                mins, secs = df.iloc[i, 10].split(\":\")\n",
    "                delta = timedelta(minutes=int(mins), seconds=float(secs))\n",
    "\n",
    "                # 종료 시각에서 재생 시간을 빼서 시작 시각 계산\n",
    "                start_time = end_time - delta\n",
    "\n",
    "                # 데이터 안정성을 위해 시작/종료 시각에 1초 마진 적용\n",
    "                margin = timedelta(seconds=1)\n",
    "                start_time_margin = start_time + margin\n",
    "                end_time_margin = end_time - margin\n",
    "\n",
    "                # 마진 적용 후 유효성 체크\n",
    "                if start_time_margin >= end_time_margin:\n",
    "                    print(f\"[⚠️] index {i}: 마진 적용 후 시작 시각이 종료 시각과 같거나 이후입니다. 무시됩니다.\")\n",
    "                    continue\n",
    "\n",
    "                # 문자열로 변환\n",
    "                start_str = datetime.strftime(start_time_margin, \"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                end_str = datetime.strftime(end_time_margin, \"%Y-%m-%d %H:%M:%S.%f\")[:-3]\n",
    "                result.append((start_str, end_str))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"index {i} 처리 중 오류: {e}\")\n",
    "\n",
    "        if not result:\n",
    "            print(\"지정된 범위 내 파싱 가능한 데이터가 없습니다.\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    # 각 구간별로 타임스탬프 추출 실행\n",
    "    start = extract_timestamps([start_range])\n",
    "    end = extract_timestamps([end_range])\n",
    "\n",
    "    low = extract_timestamps(low_range)\n",
    "    mid = extract_timestamps(mid_range)\n",
    "    high = extract_timestamps(high_range)\n",
    "\n",
    "    return start, end, low, mid, high\n",
    "\n",
    "def convert_to_unix(time_str):\n",
    "    \"\"\"\n",
    "        'YYYY-MM-DD HH:MM:SS.sss' 형식의 시간 문자열을 UNIX 타임스탬프로 변환.\n",
    "\n",
    "        Args:\n",
    "            time_str (str): 변환할 시간 문자열.\n",
    "\n",
    "        Returns:\n",
    "            int | None: 변환된 UNIX 타임스탬프 또는 변환 실패 시 None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # UNIX 타임스탬프로 변환 시도\n",
    "        dt = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S.%f')\n",
    "        #\n",
    "        return int(dt.timestamp() * 1000)\n",
    "    except ValueError:\n",
    "        print('Error')\n",
    "        return None\n",
    "\n",
    "def filter_data_by_time(df, timestamps, timestamp_column_name = 'Timestamp'):\n",
    "    \"\"\"\n",
    "        주어진 (시작, 종료) 시각 리스트에 따라 DataFrame을 필터링.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): 필터링할 원본 DataFrame.\n",
    "            timestamps (list[tuple[str, str]]): (시작 시각, 종료 시각) 문자열 튜플의 리스트.\n",
    "            timestamp_column_name (str): df에 저장되어있는 UNIX 타임스탬프의 컬럼 명.\n",
    "\n",
    "        Returns:\n",
    "            list[pd.DataFrame]: 각 타임스탬프 구간에 맞게 필터링된 DataFrame 리스트.\n",
    "    \"\"\"\n",
    "    filtered_data = []\n",
    "    # 각 (시작, 종료) 쌍에 대해 반복\n",
    "    for start, end in timestamps:\n",
    "        # 시간 문자열을 UNIX 타임스탬프로 변환\n",
    "        start_time = convert_to_unix(time_str = start)\n",
    "        end_time = convert_to_unix(time_str = end)\n",
    "        # 시작 시간과 종료 시간 사이의 데이터만 필터링하여 리스트에 추가\n",
    "        filtered_data.append(df[(df[timestamp_column_name] >= start_time) & (df[timestamp_column_name] <= end_time)])\n",
    "    return filtered_data\n",
    "\n",
    "def save_filtered_data(filtered_data, base_dir, category, prefix):\n",
    "    \"\"\"\n",
    "        필터링된 데이터프레임 리스트를 지정된 경로에 CSV 파일로 저장.\n",
    "\n",
    "        Args:\n",
    "            filtered_data (list[pd.DataFrame]): 저장할 DataFrame의 리스트.\n",
    "            base_dir (str): 저장할 최상위 경로 (e.g., output_dir).\n",
    "            category (str): 하위 폴더 이름 및 파일명 (e.g., 'low', 'mid').\n",
    "            prefix (str): 추출한 데이터의 이름 (e.g., 'PPG', 'ECG').\n",
    "    \"\"\"\n",
    "    # 저장 경로 생성 (e.g., output_dir/PPG/low)\n",
    "    dir_path = os.path.join(base_dir, prefix, category)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    # 리스트 내 각 데이터프레임을 별도 파일로 저장 (e.g., low1.csv, low2.csv, ...)\n",
    "    for idx, df in enumerate(filtered_data, start=1):\n",
    "        df.to_csv(os.path.join(dir_path, f\"{category}{idx}.csv\"), index=False)"
   ],
   "id": "971a7e4acbbc96e1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:57:01.975379Z",
     "start_time": "2025-09-30T09:55:59.467219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_data_dir = '../data/raw_data/'\n",
    "split_save_dir = '../data/split_data/'\n",
    "\n",
    "for subject_name in subject_list:\n",
    "    data_split(raw_data_dir+subject_name, split_save_dir+subject_name)\n",
    "\n",
    "data_split_result = pd.read_csv('../data/split_data/'+subject+'/ECG/high/high1.csv')\n",
    "data_split_result"
   ],
   "id": "1bf12217639abf1c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw_data/1_1_011_V2 데이터 Split 처리 시작\n",
      "../data/split_data/1_1_011_V2폴더에 Split데이터를 저장했습니다.\n",
      "\n",
      "../data/raw_data/1_1_015_V2 데이터 Split 처리 시작\n",
      "../data/split_data/1_1_015_V2폴더에 Split데이터를 저장했습니다.\n",
      "\n",
      "../data/raw_data/1_1_025_V1 데이터 Split 처리 시작\n",
      "../data/split_data/1_1_025_V1폴더에 Split데이터를 저장했습니다.\n",
      "\n",
      "../data/raw_data/1_1_027_V1 데이터 Split 처리 시작\n",
      "../data/split_data/1_1_027_V1폴더에 Split데이터를 저장했습니다.\n",
      "\n",
      "../data/raw_data/1_1_028_V1 데이터 Split 처리 시작\n",
      "../data/split_data/1_1_028_V1폴더에 Split데이터를 저장했습니다.\n",
      "\n",
      "../data/raw_data/1_1_029_V1 데이터 Split 처리 시작\n",
      "../data/split_data/1_1_029_V1폴더에 Split데이터를 저장했습니다.\n",
      "\n",
      "../data/raw_data/1_1_034_V1 데이터 Split 처리 시작\n",
      "../data/split_data/1_1_034_V1폴더에 Split데이터를 저장했습니다.\n",
      "\n",
      "../data/raw_data/1_1_035_V1 데이터 Split 처리 시작\n",
      "../data/split_data/1_1_035_V1폴더에 Split데이터를 저장했습니다.\n",
      "\n",
      "../data/raw_data/1_1_036_V1 데이터 Split 처리 시작\n",
      "../data/split_data/1_1_036_V1폴더에 Split데이터를 저장했습니다.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       Shimmer_820D_Timestamp_Unix_CAL  Shimmer_820D_ECG_EMG_Status1_CAL  \\\n",
       "0                         1.757987e+12                             128.0   \n",
       "1                         1.757987e+12                             128.0   \n",
       "2                         1.757987e+12                             128.0   \n",
       "3                         1.757987e+12                             128.0   \n",
       "4                         1.757987e+12                             128.0   \n",
       "...                                ...                               ...   \n",
       "25710                     1.757987e+12                             128.0   \n",
       "25711                     1.757987e+12                             128.0   \n",
       "25712                     1.757987e+12                             128.0   \n",
       "25713                     1.757987e+12                             128.0   \n",
       "25714                     1.757987e+12                             128.0   \n",
       "\n",
       "       Shimmer_820D_ECG_EMG_Status2_CAL  Shimmer_820D_ECG_LA-RA_24BIT_CAL  \\\n",
       "0                                 128.0                          0.750786   \n",
       "1                                 128.0                          0.745954   \n",
       "2                                 128.0                          0.751003   \n",
       "3                                 128.0                          0.743141   \n",
       "4                                 128.0                          0.745016   \n",
       "...                                 ...                               ...   \n",
       "25710                             128.0                          1.500995   \n",
       "25711                             128.0                          1.502582   \n",
       "25712                             128.0                          1.518377   \n",
       "25713                             128.0                          1.522776   \n",
       "25714                             128.0                          1.520468   \n",
       "\n",
       "       Shimmer_820D_ECG_LL-LA_24BIT_CAL  Shimmer_820D_ECG_LL-RA_24BIT_CAL  \\\n",
       "0                             -2.822913                         -2.072127   \n",
       "1                             -2.824571                         -2.078617   \n",
       "2                             -2.806613                         -2.055611   \n",
       "3                             -2.794280                         -2.051139   \n",
       "4                             -2.777692                         -2.032676   \n",
       "...                                 ...                               ...   \n",
       "25710                         -1.944471                         -0.443476   \n",
       "25711                         -1.959977                         -0.457395   \n",
       "25712                         -1.949087                         -0.430710   \n",
       "25713                         -1.924926                         -0.402150   \n",
       "25714                         -1.897087                         -0.376619   \n",
       "\n",
       "       Shimmer_820D_ECG_Vx-RL_24BIT_CAL  Unnamed: 7  \n",
       "0                            -12.091840         NaN  \n",
       "1                            -12.066670         NaN  \n",
       "2                            -12.115280         NaN  \n",
       "3                            -12.092057         NaN  \n",
       "4                            -12.106048         NaN  \n",
       "...                                 ...         ...  \n",
       "25710                        -11.867542         NaN  \n",
       "25711                        -11.846338         NaN  \n",
       "25712                        -11.874249         NaN  \n",
       "25713                        -11.900934         NaN  \n",
       "25714                        -11.899420         NaN  \n",
       "\n",
       "[25715 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shimmer_820D_Timestamp_Unix_CAL</th>\n",
       "      <th>Shimmer_820D_ECG_EMG_Status1_CAL</th>\n",
       "      <th>Shimmer_820D_ECG_EMG_Status2_CAL</th>\n",
       "      <th>Shimmer_820D_ECG_LA-RA_24BIT_CAL</th>\n",
       "      <th>Shimmer_820D_ECG_LL-LA_24BIT_CAL</th>\n",
       "      <th>Shimmer_820D_ECG_LL-RA_24BIT_CAL</th>\n",
       "      <th>Shimmer_820D_ECG_Vx-RL_24BIT_CAL</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.757987e+12</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.750786</td>\n",
       "      <td>-2.822913</td>\n",
       "      <td>-2.072127</td>\n",
       "      <td>-12.091840</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.757987e+12</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.745954</td>\n",
       "      <td>-2.824571</td>\n",
       "      <td>-2.078617</td>\n",
       "      <td>-12.066670</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.757987e+12</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.751003</td>\n",
       "      <td>-2.806613</td>\n",
       "      <td>-2.055611</td>\n",
       "      <td>-12.115280</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.757987e+12</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.743141</td>\n",
       "      <td>-2.794280</td>\n",
       "      <td>-2.051139</td>\n",
       "      <td>-12.092057</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.757987e+12</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.745016</td>\n",
       "      <td>-2.777692</td>\n",
       "      <td>-2.032676</td>\n",
       "      <td>-12.106048</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25710</th>\n",
       "      <td>1.757987e+12</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.500995</td>\n",
       "      <td>-1.944471</td>\n",
       "      <td>-0.443476</td>\n",
       "      <td>-11.867542</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25711</th>\n",
       "      <td>1.757987e+12</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.502582</td>\n",
       "      <td>-1.959977</td>\n",
       "      <td>-0.457395</td>\n",
       "      <td>-11.846338</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25712</th>\n",
       "      <td>1.757987e+12</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.518377</td>\n",
       "      <td>-1.949087</td>\n",
       "      <td>-0.430710</td>\n",
       "      <td>-11.874249</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25713</th>\n",
       "      <td>1.757987e+12</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.522776</td>\n",
       "      <td>-1.924926</td>\n",
       "      <td>-0.402150</td>\n",
       "      <td>-11.900934</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25714</th>\n",
       "      <td>1.757987e+12</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.520468</td>\n",
       "      <td>-1.897087</td>\n",
       "      <td>-0.376619</td>\n",
       "      <td>-11.899420</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25715 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2-2. Data Label Generation\n",
    "---\n",
    "### Function get_label\n",
    "\n",
    "##### - 한 Subject의 음주 갈망 설문(SAM) 결과를 Excel 파일에서 찾아 처리하고, 결과를 파일별 라벨이 담긴 CSV로 저장.\n",
    "- Args:\n",
    "    - base_dir (str): Subject의 raw_data 경로 VR_Timestamp를 확보하기 위함.\n",
    "    - sam_result_path (str): 강동성심병원(KD.xlsx)와 춘천성심병원(CC.xlsx) 설문 결과 파일이 들어있는 디렉토리 경로.\n",
    "    - subject_path (str): 처리할 Subject의 이름 또는 ID.\n",
    "    - output_dir (str): 최종 라벨 CSV 파일을 저장할 디렉토리 경로.\n",
    "- Returns:\n",
    "    - pd.dataframe | None: 한 Subject의 라벨값이 들어있는 dataframe 오류가 발생하면 None 값 전송\n",
    "\n",
    "\n",
    "### Function subject_SAM_result\n",
    "##### - DataFrame에서 특정 Subject의 SAM 설문 결과를 찾아 trial별로 처리.\n",
    "\n",
    "- Args:\n",
    "    - data (pd.DataFrame): 설문 결과가 담긴 DataFrame (Excel 시트에서 로드).\n",
    "    - subject_name (str): 찾을 Subject의 이름.\n",
    "    - subject_check (int, optional): 그룹을 구분하는 플래그. 0: 알코올 그룹, 1: 대조군.\n",
    "- Returns:\n",
    "    - list:\n",
    "        - 처리된 라벨 리스트.\n",
    "        - (subject_more, subject_over, subject_avg, subject_Q1, subject_Q2)\n",
    "        - Subject를 찾지 못하거나 오류 발생 시 (None, None, None, None, None) 반환."
   ],
   "id": "808c80d35dcd81fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:57:02.312425Z",
     "start_time": "2025-09-30T09:57:02.287706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "def get_label(base_dir: str, sam_result_path: str, subject_path: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    한 Subject의 음주 갈망 설문(SAM) 결과를 Excel 파일에서 찾아 처리하고, 결과를 파일별 라벨이 담긴 CSV로 저장.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Subject의 raw_data 경로 VR_Timestamp를 확보하기 위함.\n",
    "        sam_result_path (str): 강동성심병원(KD.xlsx)와 춘천성심병원(CC.xlsx) 설문 결과 파일이 들어있는 디렉토리 경로.\n",
    "        subject_path (str): 처리할 Subject의 이름 또는 ID.\n",
    "        output_dir (str): 최종 라벨 CSV 파일을 저장할 디렉토리 경로.\n",
    "\n",
    "    Returns:\n",
    "        pd.dataframe | None: 한 Subject의 라벨값이 들어있는 dataframe 오류가 발생하면 None 값 전송\n",
    "    \"\"\"\n",
    "\n",
    "    # 경로 및 라벨 변수 초기화\n",
    "    VR_timestamp_path = ''\n",
    "    trial_label_more = None\n",
    "    trial_label_over = None\n",
    "    trial_label_avg = None\n",
    "    Q1 = None\n",
    "    Q2 = None\n",
    "\n",
    "    print(base_dir+' Label 정보 처리 시작')\n",
    "\n",
    "    # 필수 데이터 파일 경로 탐색 (VT_Timestamp)\n",
    "    if 'VR_Timestamp' in os.listdir(base_dir):\n",
    "        VR_timestamp_path = base_dir + '/VR_Timestamp/' + os.listdir(base_dir + '/VR_Timestamp')[0]\n",
    "    elif VR_timestamp_path == '':\n",
    "        print('폴더 내 VR_timestamp 파일이 없습니다.')\n",
    "        return\n",
    "\n",
    "    # Subject 정보 파싱 및 라벨 파일 로드\n",
    "    subject_split = subject_path.split('_')\n",
    "    subject_path = subject_path.split('(')\n",
    "    subject_path = subject_path[0]\n",
    "\n",
    "    # 대조군과 알콜환자 및 병원 분류 후 라벨 데이터 로드\n",
    "    # 알코올 그룹\n",
    "    if subject_split[0] == '1':\n",
    "        # 강동성심병원\n",
    "        if subject_split[1] == '1':\n",
    "            data = pd.read_excel(sam_result_path+r'\\KD.xlsx', sheet_name='알코올음주갈망')\n",
    "            data.drop(21,inplace=True)\n",
    "            trial_label_more, trial_label_over, trial_label_avg, Q1, Q2 = subject_SAM_result(data, subject_path, subject_check=0)\n",
    "        # 춘천성심병원\n",
    "        elif subject_split[1] == '2':\n",
    "            data = pd.read_excel(sam_result_path+r'\\CC.xlsx', sheet_name='알코올음주갈망')\n",
    "            data = data[2:]\n",
    "            trial_label_more, trial_label_over, trial_label_avg, Q1, Q2 = subject_SAM_result(data, subject_path, subject_check=0)\n",
    "    # 대조군 그룹\n",
    "    if subject_split[0] == '3':\n",
    "        # 강동성심병원\n",
    "        if subject_split[1] == '1':\n",
    "            data = pd.read_excel(sam_result_path+r'\\KD.xlsx', sheet_name='대조군음주갈망')\n",
    "            trial_label_more, trial_label_over, trial_label_avg, Q1, Q2 = subject_SAM_result(data, subject_path, subject_check=1)\n",
    "        # 춘천성심병원\n",
    "        if subject_split[1] == '2':\n",
    "            data = pd.read_excel(sam_result_path+r'\\CC.xlsx', sheet_name='대조군음주갈망')\n",
    "            trial_label_more, trial_label_over, trial_label_avg, Q1, Q2 = subject_SAM_result(data, subject_path, subject_check=1)\n",
    "\n",
    "    # 라벨 데이터 유효성 검사\n",
    "    if trial_label_over == None or trial_label_more == None or trial_label_avg == None or Q1 == None or Q2 == None:\n",
    "        print(subject_path + '의 라벨값이 없습니다.')\n",
    "        return None\n",
    "\n",
    "    # VR 타임스탬프에서 low, mid, high 영상 개수 확인\n",
    "    start, end, low, mid, high = get_VR_timestamp(file_path=VR_timestamp_path)\n",
    "\n",
    "    # 영상 파일명 리스트 생성\n",
    "    label = []\n",
    "    for i in range(len(low)):\n",
    "        label.append('low'+str(i+1)+'.csv')\n",
    "    for i in range(len(mid)):\n",
    "        label.append('mid'+str(i+1)+'.csv')\n",
    "    for i in range(len(high)):\n",
    "        label.append('high'+str(i+1)+'.csv')\n",
    "    # 영상 파일 개수와 추출된 라벨 개수가 일치하는지 확인\n",
    "    if len(label) != len(trial_label_over) or len(label) != len(trial_label_more) or len(label) != len(trial_label_avg) or len(label) != len(Q1) or len(label) != len(Q2):\n",
    "        print(subject_path+'에 누락된 라벨이 있습니다.')\n",
    "        return None\n",
    "\n",
    "    # 최종 데이터프레임 생성 및 저장\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    df[\"File\"] = label\n",
    "    # 3.5점 이상이면 1\n",
    "    df[\"label-3.5_more\"] = trial_label_more\n",
    "    # 3.5점 초과이면 1\n",
    "    df[\"label-3.5_over\"] = trial_label_over\n",
    "    # Q1, Q2 평균값\n",
    "    df[\"label-avg\"] = trial_label_avg\n",
    "    # Q1 값\n",
    "    df['Q1'] = Q1\n",
    "    # Q2 값\n",
    "    df['Q2'] = Q2\n",
    "\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        save_file = os.path.join(output_dir, subject_path + '.csv')\n",
    "        df.to_csv(save_file, index=False)\n",
    "        print(f\"{save_file}에 Label 정보를 저장했습니다.\\n\")\n",
    "    return df\n",
    "\n",
    "def subject_SAM_result(data, subject_name, subject_check=0):\n",
    "    \"\"\"\n",
    "    DataFrame에서 특정 Subject의 SAM 설문 결과를 찾아 trial별로 처리.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): 설문 결과가 담긴 DataFrame (Excel 시트에서 로드).\n",
    "        subject_name (str): 찾을 Subject의 이름.\n",
    "        subject_check (int, optional): 그룹을 구분하는 플래그. 0: 알코올 그룹, 1: 대조군.\n",
    "\n",
    "    Returns:\n",
    "        list:\n",
    "            처리된 라벨 리스트.\n",
    "            (subject_more, subject_over, subject_avg, subject_Q1, subject_Q2)\n",
    "            Subject를 찾지 못하거나 오류 발생 시 (None, None, None, None, None) 반환.\n",
    "    \"\"\"\n",
    "    # 데이터 전처리 (필요 없는 정보 제거)\n",
    "    data.drop(axis=1, labels=['참여자 ID', 'Visit 세션'], inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 결과를 저장할 리스트 초기화\n",
    "    subject_avg = []\n",
    "    subject_more = []\n",
    "    subject_over = []\n",
    "    subject_Q1 = []\n",
    "    subject_Q2 = []\n",
    "\n",
    "    # 그룹(알코올/대조군)에 따라 설문 점수가 시작되는 컬럼 인덱스 설정\n",
    "    if subject_check == 0:\n",
    "        q1 = 2\n",
    "        q2 = 25\n",
    "    else:\n",
    "        q1 = 2\n",
    "        q2 = 20\n",
    "\n",
    "    # DataFrame을 순회하며 일치하는 Subject 탐색\n",
    "    for i in range(len(data)):\n",
    "        temp = data.iloc[i]\n",
    "        if temp[0] == subject_name:\n",
    "            if np.isnan(temp[2]) == False:\n",
    "                # Subject를 찾으면, trial별 점수 추출\n",
    "                for i in range(18):\n",
    "                    # 데이터가 '-'와 같은 비숫자 값이면 중단\n",
    "                    if temp[i+q1] == '-' or temp[i+q2] == '-':\n",
    "                        break\n",
    "                    try:\n",
    "\n",
    "                        # 5. 라벨 계산 및 리스트에 추가\n",
    "                        subject_avg.append((int(temp[i+q1]) + int(temp[i+q2])) / 2)\n",
    "                        subject_more.append(1 if((int(temp[i+q1]) + int(temp[i+q2])) / 2) >= 3.5 else 0)\n",
    "                        subject_over.append(1 if((int(temp[i+q1]) + int(temp[i+q2])) / 2) > 3.5 else 0)\n",
    "                        subject_Q1.append(int(temp[i+q1]))\n",
    "                        subject_Q2.append(int(temp[i+q2]))\n",
    "                    except:\n",
    "                        # 변환 중 오류 발생 시 None 반환\n",
    "                        return None, None, None, None, None\n",
    "                # Subject의 모든 trial 처리가 끝나면 결과 반환\n",
    "                return subject_more, subject_over, subject_avg, subject_Q1, subject_Q2\n",
    "     # 6. DataFrame 전체를 순회해도 Subject를 찾지 못한 경우\n",
    "    return None, None, None, None, None"
   ],
   "id": "f4108fe27165e78",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:57:03.310254Z",
     "start_time": "2025-09-30T09:57:02.385644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_dir = r'../data/raw_data/'\n",
    "sam_result_path = r'../data/SAM/'\n",
    "save_dir = '../features/label/'\n",
    "Label_result = get_label(base_dir=base_dir+subject, sam_result_path=sam_result_path, subject_path=subject, output_dir=save_dir)\n",
    "Label_result"
   ],
   "id": "56fe8cdf7d45a6c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/raw_data/1_1_011_V2 Label 정보 처리 시작\n",
      "../features/label/1_1_011_V2.csv에 Label 정보를 저장했습니다.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         File  label-3.5_more  label-3.5_over  label-avg  Q1  Q2\n",
       "0    low1.csv               0               0        0.0   0   0\n",
       "1    low2.csv               0               0        0.0   0   0\n",
       "2    low3.csv               0               0        0.0   0   0\n",
       "3    low4.csv               0               0        0.0   0   0\n",
       "4    low5.csv               0               0        0.0   0   0\n",
       "5    low6.csv               0               0        0.0   0   0\n",
       "6    mid1.csv               0               0        0.0   0   0\n",
       "7    mid2.csv               0               0        1.5   3   0\n",
       "8    mid3.csv               0               0        0.0   0   0\n",
       "9    mid4.csv               0               0        0.0   0   0\n",
       "10   mid5.csv               0               0        0.0   0   0\n",
       "11  high1.csv               0               0        0.0   0   0\n",
       "12  high2.csv               0               0        0.5   1   0\n",
       "13  high3.csv               0               0        0.0   0   0\n",
       "14  high4.csv               0               0        1.0   2   0\n",
       "15  high5.csv               0               0        0.0   0   0\n",
       "16  high6.csv               0               0        1.0   2   0\n",
       "17  high7.csv               0               0        0.0   0   0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>label-3.5_more</th>\n",
       "      <th>label-3.5_over</th>\n",
       "      <th>label-avg</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low1.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>low2.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>low3.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>low4.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>low5.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>low6.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mid1.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mid2.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mid3.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mid4.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mid5.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>high1.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>high2.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>high3.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>high4.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>high5.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>high6.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>high7.csv</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. 생체신호 특징 추출\n",
    "## 3-1. ECG, PPG HR_HRV\n",
    "---\n",
    "### Function extract_signal\n",
    "\n",
    "##### - ECG, PPG, GSR의 signal을 dataframe으로 저장.\n",
    "- Args:\n",
    "    - df (Dataframe): CSV 파일 이름 (예: \"sub1.csv\").\n",
    "    - data_type (str): 추출해야할 signal의 이름.\n",
    "- Returns:\n",
    "    - dict[channel_name : signal]\n",
    "        - signal(dict): 각 데이터 타입에 맞는 signal 반환.\n",
    "        - data_type에 올바르지 않은 data type이 들어 있다면 빈 dict 반환"
   ],
   "id": "28ddc225b7f79151"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:57:03.691513Z",
     "start_time": "2025-09-30T09:57:03.682565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_signal(df, data_type = 'ECG'):\n",
    "    \"\"\"\n",
    "    데이터프레임에서 생체 신호 추출.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Shimmer 센서 데이터가 포함된 원본 데이터프레임.\n",
    "        data_type (str): 추출할 신호의 종류. 'ECG', 'PPG', 'GSR' 중 하나를 선택.\n",
    "\n",
    "    Returns:\n",
    "        dict: 채널 이름을 키(key)로, 해당 신호(signal)를 값(value)으로 갖는 딕셔너리.\n",
    "              만약 data_type이 유효하지 않으면 빈 딕셔너리를 반환.\n",
    "    \"\"\"\n",
    "\n",
    "    columns = {\n",
    "        'ECG': {\n",
    "            'LA_RA' : 'Shimmer_820D_ECG_LA-RA_24BIT_CAL',\n",
    "            'LL_LA' : 'Shimmer_820D_ECG_LL-LA_24BIT_CAL',\n",
    "            'LL_RA' : 'Shimmer_820D_ECG_LL-RA_24BIT_CAL',\n",
    "            'Vx_RL' : 'Shimmer_820D_ECG_Vx-RL_24BIT_CAL'\n",
    "        },\n",
    "        'PPG' : {\n",
    "            'ppg' : 'id95AE_PPG_A13_CAL'\n",
    "        },\n",
    "        'GSR' : {\n",
    "            'gsr' : 'id95AE_GSR_Skin_Conductance_CAL'\n",
    "\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # df에서 신호를 추출\n",
    "    signal = {key: df[col].values for key, col in columns[data_type].items()}\n",
    "    return signal"
   ],
   "id": "a605ab30a6a1e87a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3-1-1. ECG HR_HRV\n",
    "---\n",
    "### Function ECG_metrics\n",
    "##### - 한 Subject의 모든 ECG 데이터를 분석하여 HR, HRV 지표를 계산하고 결과를 반환.\n",
    "- Args:\n",
    "    - subject_ECG_dir_path (str): 분석할 Subject의 ECG 데이터가 담긴 상위 디렉토리 경로. (e.g: '.../data/sub1/ECG')\n",
    "    - save_path (str, optional): 분석 결과(CSV, plot)를 저장할 최상위 디렉토리 경로.\n",
    "    - show (bool, optional): 분석 과정에서 생성되는 plot을 화면에 표시할지 여부. save_path가 지정된 경우에만 활성화.\n",
    "    - unit (str, optional): ECG 신호의 단위 (예: 'mV'). Plot 제목에 사용.\n",
    "- Returns:\n",
    "    - pd.DataFrame: Subject의 모든 파일과 채널에 대한 HR, HRV 분석 결과가 포함된 데이터프레임.\n",
    "\n",
    "### Function save_ECG_HR_plot\n",
    "##### - ECG 신호를 분석하여 평균 심박수(HR)를 계산하고, 관련 Plot 저장.\n",
    "- Args:\n",
    "    - ecg_signal (np.ndarray): 분석할 Raw ECG 신호.\n",
    "    - show (bool, optional): 분석 Plot을 화면에 표시할지 여부.\n",
    "    - title (str, optional): Plot에 표시될 제목.\n",
    "    - save_path (str, optional): Plot을 저장할 경로 및 파일명.\n",
    "    - unit (str, optional): 신호의 단위 (y축 라벨에 사용).\n",
    "- Returns:\n",
    "    - float: 계산된 평균 심박수(ECG_Rate_Mean). 오류 발생 시 0을 반환.\n",
    "\n",
    "### Function save_ECG_HRV_plot\n",
    "##### - ECG 신호에서 심박 변이도(HRV) 지표들을 계산하고, 관련 Plot 저장.\n",
    "- Args:\n",
    "    - ecg_signal (np.ndarray): 분석할 Raw ECG 신호.\n",
    "    - show (bool, optional): 분석 Plot을 화면에 표시할지 여부.\n",
    "    - title (str, optional): Plot에 표시될 제목.\n",
    "    - save_path (str, optional): Plot을 저장할 경로 및 파일명.\n",
    "- Returns:\n",
    "    - pd.DataFrame | None: 계산된 모든 HRV 지표가 포함된 DataFrame. 오류 발생 시 None을 반환.\n"
   ],
   "id": "94891c0720a04955"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:57:04.026072Z",
     "start_time": "2025-09-30T09:57:04.006766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NeuroKit2에서 발생하는 특정 경고만 무시하도록 설정\n",
    "warnings.filterwarnings(\"ignore\", category=NeuroKitWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "def ECG_metrics(subject_ECG_dir_path, save_path=None, show=False, unit=None):\n",
    "    \"\"\"\n",
    "    한 Subject의 모든 ECG 데이터를 분석하여 HR, HRV 지표를 계산하고 결과를 반환.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        subject_ECG_dir_path (str): 분석할 Subject의 ECG 데이터가 담긴 상위 디렉토리 경로. (e.g: '.../data/sub1/ECG')\n",
    "        save_path (str, optional): 분석 결과(CSV, plot)를 저장할 최상위 디렉토리 경로.\n",
    "        show (bool, optional): 분석 과정에서 생성되는 plot을 화면에 표시할지 여부. save_path가 지정된 경우에만 활성화.\n",
    "        unit (str, optional): ECG 신호의 단위 (예: 'mV'). Plot 제목에 사용.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            Subject의 모든 파일과 채널에 대한 HR, HRV 분석 결과가 포함된 데이터프레임.\n",
    "    \"\"\"\n",
    "    print(subject_ECG_dir_path+ ' ECG Signal - HR, HRV 분석 시작')\n",
    "    # 최종 결과를 저장하기 위한 빈 데이터프레임 생성\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # subject 내 ECG 디렉토리 파일들 순회 (low, mid, high)\n",
    "    for dir_name in os.listdir(subject_ECG_dir_path):\n",
    "        dir_path = os.path.join(subject_ECG_dir_path, dir_name)\n",
    "\n",
    "        # 데이터 저장을 위한 Subject 이름 추출\n",
    "        subject_path = subject_ECG_dir_path.split('\\\\')[3]\n",
    "        # 시각화를 원한다면 시각화 결과를 저장할 폴더 생성\n",
    "        if save_path and show:\n",
    "            os.makedirs(save_path + '/' + subject_path, exist_ok=True)\n",
    "\n",
    "        # ECG 데이터 파일 순회 (low1.csv, low2.csv, ...)\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "            # 파일명에서 확장자를 제외한 부분 추출 (e.g., 'low1')\n",
    "            title_name = file_name.split('.')[0]\n",
    "\n",
    "            # CSV 파일에서 ECG 신호 추출\n",
    "            raw_signal = extract_signal(pd.read_csv(file_path), 'ECG')\n",
    "\n",
    "            # 한 파일의 분석 결과를 임시로 담을 데이터프레임 생성\n",
    "            temp = pd.DataFrame()\n",
    "            temp[\"File\"] = [file_name]\n",
    "\n",
    "            # 추출된 신호의 각 채널별로 처리\n",
    "            for channel_name, raw in raw_signal.items():\n",
    "                temp[\"channel\"] = channel_name\n",
    "\n",
    "                # Plot에 사용할 제목 생성\n",
    "                title = '' + title_name + ' - ECG ' + channel_name\n",
    "                # HR(심박수) 계산 및 Plot 저장\n",
    "                HR = save_ECG_HR_plot(raw, show, title=title,\n",
    "                                      save_path=save_path + '/' + subject_path + '/' + title_name + ' - ECG ' + channel_name if save_path and show else None,\n",
    "                                      unit=unit)\n",
    "                temp[\"HR\"] = HR\n",
    "\n",
    "                # HRV(심박 변이도) 지표 계산 및 Plot 저장\n",
    "                HRV = save_ECG_HRV_plot(raw, show, title=title,\n",
    "                                        save_path=save_path + '/' + subject_path + '/' + title_name + ' - ECG ' + channel_name if save_path and show else None)\n",
    "\n",
    "                # HR과 HRV 결과를 임시 데이터프레임에 병합\n",
    "                result = pd.concat([temp, HRV], axis=1)\n",
    "\n",
    "                # 최종 데이터프레임에 현재 채널의 결과 누적\n",
    "                if df.size == 0:\n",
    "                    df = result\n",
    "                else:\n",
    "                    df = pd.concat([df, result], ignore_index=True)\n",
    "\n",
    "    # 최종 결과를 CSV 파일로 저장 (save_path가 지정된 경우)\n",
    "    if save_path:\n",
    "        # 저장 경로가 없으면 생성\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        # CSV로 저장\n",
    "        save_file = os.path.join(save_path, subject_ECG_dir_path.split('\\\\')[-2] + '.csv')\n",
    "        df.to_csv(save_file, index=False)\n",
    "\n",
    "        # 파일 저장 완료 메시지 출력\n",
    "        print(f\"{save_file}에 ECG Signal - HR, HRV 분석 결과를 저장했습니다.\")\n",
    "    # 분석 결과가 담긴 최종 데이터프레임 반환\n",
    "    return df\n",
    "\n",
    "def save_ECG_HR_plot(ecg_signal, show=False, title=None, save_path=None, unit=None):\n",
    "    \"\"\"\n",
    "        ECG 신호를 분석하여 평균 심박수(HR)를 계산하고, 관련 Plot 저장.\n",
    "\n",
    "        Args:\n",
    "            ecg_signal (np.ndarray): 분석할 Raw ECG 신호.\n",
    "            show (bool, optional): 분석 Plot을 화면에 표시할지 여부.\n",
    "            title (str, optional): Plot에 표시될 제목.\n",
    "            save_path (str, optional): Plot을 저장할 경로 및 파일명.\n",
    "            unit (str, optional): 신호의 단위 (y축 라벨에 사용).\n",
    "\n",
    "        Returns:\n",
    "            float: 계산된 평균 심박수(ECG_Rate_Mean). 오류 발생 시 0을 반환.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ECG 전처리 및 분석\n",
    "        signals, info = nk.ecg_process(ecg_signal, sampling_rate=FS_ECG)\n",
    "        analyze_df = nk.ecg_analyze(signals, sampling_rate=FS_ECG)\n",
    "\n",
    "        if save_path:\n",
    "            nk.ecg_plot(signals, info, title + ' (HR)', save_path + ' (HR)', unit, show=show)\n",
    "        # 시각화 옵션 (show = True)\n",
    "\n",
    "        # 평균 심박수 추출\n",
    "        try:\n",
    "            mean_hr = analyze_df['ECG_Rate_Mean'].values[0]\n",
    "            # print(f\"Calculated HR: {mean_hr}\")  # HR 값을 출력\n",
    "        except KeyError:\n",
    "            print(\"Error: 'ECG_Rate_Mean' not found in analysis.\")\n",
    "            mean_hr = None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        mean_hr = 0\n",
    "    return mean_hr\n",
    "\n",
    "def save_ECG_HRV_plot(ecg_signal, show=False, title=None, save_path=None):\n",
    "    \"\"\"\n",
    "        ECG 신호에서 심박 변이도(HRV) 지표들을 계산하고, 관련 Plot 저장.\n",
    "\n",
    "        Args:\n",
    "            ecg_signal (np.ndarray): 분석할 Raw ECG 신호.\n",
    "            show (bool, optional): 분석 Plot을 화면에 표시할지 여부.\n",
    "            title (str, optional): Plot에 표시될 제목.\n",
    "            save_path (str, optional): Plot을 저장할 경로 및 파일명.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame | None:\n",
    "                계산된 모든 HRV 지표가 포함된 DataFrame. 오류 발생 시 None을 반환.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # print(\"Signal length:\", len(ecg_signal))\n",
    "\n",
    "        # ECG 전처리 및 분석 peak 추출\n",
    "        signals, info = nk.ecg_process(ecg_signal, sampling_rate=FS_ECG)\n",
    "        peaks, info = nk.ecg_peaks(signals, sampling_rate=FS_ECG)\n",
    "\n",
    "        # 검출된 peak를 기반으로 HRV 지표 계산 및 시각화\n",
    "        hrv = nk.hrv(peaks, sampling_rate=FS_ECG, show=show, title=title + ' (HRV)', save_path=save_path + ' (HRV)' if save_path else None)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return hrv"
   ],
   "id": "fe7fdc2fdeef4993",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:58:42.591719Z",
     "start_time": "2025-09-30T09:57:04.115972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_data_dir = r\"..\\data\\split_data\"\n",
    "save_path = \"..\\\\features\\\\HR_HRV\\\\ECG\\\\\"+subject\n",
    "# Save\n",
    "ECG_HR_HRV_result = ECG_metrics(os.path.join(split_data_dir, subject)+\"\\\\ECG\", save_path=save_path)\n",
    "ECG_HR_HRV_result"
   ],
   "id": "9b0d4349b2565006",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\split_data\\1_1_011_V2\\ECG ECG Signal - HR, HRV 분석 시작\n",
      "..\\features\\HR_HRV\\ECG\\1_1_011_V2\\1_1_011_V2.csv에 ECG Signal - HR, HRV 분석 결과를 저장했습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          File channel         HR  HRV_MeanNN   HRV_SDNN  HRV_SDANN1  \\\n",
       "0     end1.csv   LA_RA  78.350726  765.755208  29.129826         NaN   \n",
       "1     end1.csv   LL_LA  78.352829  765.755208  29.184634         NaN   \n",
       "2     end1.csv   LL_RA  78.356089  765.729167  29.164420         NaN   \n",
       "3     end1.csv   Vx_RL  78.350560  765.755208  29.136904         NaN   \n",
       "4    high1.csv   LA_RA  74.849269  801.869877  31.661364         NaN   \n",
       "..         ...     ...        ...         ...        ...         ...   \n",
       "75    mid5.csv   Vx_RL  73.241312  819.295247  27.488780         NaN   \n",
       "76  start1.csv   LA_RA  79.730734  752.621299  59.158350         NaN   \n",
       "77  start1.csv   LL_LA  79.725996  752.646998  59.213480         NaN   \n",
       "78  start1.csv   LL_RA  79.726315  752.646998  59.228939         NaN   \n",
       "79  start1.csv   Vx_RL  79.727565  752.646998  59.149022         NaN   \n",
       "\n",
       "    HRV_SDNNI1  HRV_SDANN2  HRV_SDNNI2  HRV_SDANN5  ...  HRV_SampEn  \\\n",
       "0          NaN         NaN         NaN         NaN  ...    1.491655   \n",
       "1          NaN         NaN         NaN         NaN  ...    2.484907   \n",
       "2          NaN         NaN         NaN         NaN  ...    1.763589   \n",
       "3          NaN         NaN         NaN         NaN  ...    2.427748   \n",
       "4          NaN         NaN         NaN         NaN  ...    1.658228   \n",
       "..         ...         ...         ...         ...  ...         ...   \n",
       "75         NaN         NaN         NaN         NaN  ...         inf   \n",
       "76         NaN         NaN         NaN         NaN  ...    1.172720   \n",
       "77         NaN         NaN         NaN         NaN  ...    1.247032   \n",
       "78         NaN         NaN         NaN         NaN  ...    1.187843   \n",
       "79         NaN         NaN         NaN         NaN  ...    1.120591   \n",
       "\n",
       "    HRV_ShanEn  HRV_FuzzyEn  HRV_MSEn  HRV_CMSEn  HRV_RCMSEn    HRV_CD  \\\n",
       "0     5.081631     1.368162  1.275449   0.807091    1.337719  1.760663   \n",
       "1     5.211957     1.354527  1.257609   0.983438    1.361536  1.778267   \n",
       "2     5.223698     1.344070  1.309441   0.856398    1.278211  1.784201   \n",
       "3     5.197031     1.354882  0.953931   1.217621    1.500527  1.842993   \n",
       "4     4.938453     1.310519  1.362404   0.927903    1.178831  1.396763   \n",
       "..         ...          ...       ...        ...         ...       ...   \n",
       "75    4.761842     1.496542       NaN   0.000000    0.000000  2.491066   \n",
       "76    5.691814     0.868742  0.871618   1.019680    1.343463  1.715302   \n",
       "77    5.612866     0.870433  0.624949   0.968962    1.261006  1.672951   \n",
       "78    5.675430     0.868395  0.985380   0.974733    1.220224  1.736408   \n",
       "79    5.612866     0.864282  0.902979   0.941243    1.210625  1.714994   \n",
       "\n",
       "     HRV_HFD   HRV_KFD   HRV_LZC  \n",
       "0   1.862438  3.145242  1.245764  \n",
       "1   1.866197  3.145242  1.162713  \n",
       "2   1.867414  3.094883  1.245764  \n",
       "3   1.863170  3.075252  1.245764  \n",
       "4   1.838432  3.496750  1.069477  \n",
       "..       ...       ...       ...  \n",
       "75  1.940085  4.592174  1.163534  \n",
       "76  1.478053  1.826775  0.575467  \n",
       "77  1.480150  1.823616  0.575467  \n",
       "78  1.483196  1.824611  0.575467  \n",
       "79  1.473518  1.819468  0.575467  \n",
       "\n",
       "[80 rows x 85 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>channel</th>\n",
       "      <th>HR</th>\n",
       "      <th>HRV_MeanNN</th>\n",
       "      <th>HRV_SDNN</th>\n",
       "      <th>HRV_SDANN1</th>\n",
       "      <th>HRV_SDNNI1</th>\n",
       "      <th>HRV_SDANN2</th>\n",
       "      <th>HRV_SDNNI2</th>\n",
       "      <th>HRV_SDANN5</th>\n",
       "      <th>...</th>\n",
       "      <th>HRV_SampEn</th>\n",
       "      <th>HRV_ShanEn</th>\n",
       "      <th>HRV_FuzzyEn</th>\n",
       "      <th>HRV_MSEn</th>\n",
       "      <th>HRV_CMSEn</th>\n",
       "      <th>HRV_RCMSEn</th>\n",
       "      <th>HRV_CD</th>\n",
       "      <th>HRV_HFD</th>\n",
       "      <th>HRV_KFD</th>\n",
       "      <th>HRV_LZC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end1.csv</td>\n",
       "      <td>LA_RA</td>\n",
       "      <td>78.350726</td>\n",
       "      <td>765.755208</td>\n",
       "      <td>29.129826</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.491655</td>\n",
       "      <td>5.081631</td>\n",
       "      <td>1.368162</td>\n",
       "      <td>1.275449</td>\n",
       "      <td>0.807091</td>\n",
       "      <td>1.337719</td>\n",
       "      <td>1.760663</td>\n",
       "      <td>1.862438</td>\n",
       "      <td>3.145242</td>\n",
       "      <td>1.245764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>end1.csv</td>\n",
       "      <td>LL_LA</td>\n",
       "      <td>78.352829</td>\n",
       "      <td>765.755208</td>\n",
       "      <td>29.184634</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>5.211957</td>\n",
       "      <td>1.354527</td>\n",
       "      <td>1.257609</td>\n",
       "      <td>0.983438</td>\n",
       "      <td>1.361536</td>\n",
       "      <td>1.778267</td>\n",
       "      <td>1.866197</td>\n",
       "      <td>3.145242</td>\n",
       "      <td>1.162713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>end1.csv</td>\n",
       "      <td>LL_RA</td>\n",
       "      <td>78.356089</td>\n",
       "      <td>765.729167</td>\n",
       "      <td>29.164420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.763589</td>\n",
       "      <td>5.223698</td>\n",
       "      <td>1.344070</td>\n",
       "      <td>1.309441</td>\n",
       "      <td>0.856398</td>\n",
       "      <td>1.278211</td>\n",
       "      <td>1.784201</td>\n",
       "      <td>1.867414</td>\n",
       "      <td>3.094883</td>\n",
       "      <td>1.245764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>end1.csv</td>\n",
       "      <td>Vx_RL</td>\n",
       "      <td>78.350560</td>\n",
       "      <td>765.755208</td>\n",
       "      <td>29.136904</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.427748</td>\n",
       "      <td>5.197031</td>\n",
       "      <td>1.354882</td>\n",
       "      <td>0.953931</td>\n",
       "      <td>1.217621</td>\n",
       "      <td>1.500527</td>\n",
       "      <td>1.842993</td>\n",
       "      <td>1.863170</td>\n",
       "      <td>3.075252</td>\n",
       "      <td>1.245764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high1.csv</td>\n",
       "      <td>LA_RA</td>\n",
       "      <td>74.849269</td>\n",
       "      <td>801.869877</td>\n",
       "      <td>31.661364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.658228</td>\n",
       "      <td>4.938453</td>\n",
       "      <td>1.310519</td>\n",
       "      <td>1.362404</td>\n",
       "      <td>0.927903</td>\n",
       "      <td>1.178831</td>\n",
       "      <td>1.396763</td>\n",
       "      <td>1.838432</td>\n",
       "      <td>3.496750</td>\n",
       "      <td>1.069477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>mid5.csv</td>\n",
       "      <td>Vx_RL</td>\n",
       "      <td>73.241312</td>\n",
       "      <td>819.295247</td>\n",
       "      <td>27.488780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>4.761842</td>\n",
       "      <td>1.496542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.491066</td>\n",
       "      <td>1.940085</td>\n",
       "      <td>4.592174</td>\n",
       "      <td>1.163534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>start1.csv</td>\n",
       "      <td>LA_RA</td>\n",
       "      <td>79.730734</td>\n",
       "      <td>752.621299</td>\n",
       "      <td>59.158350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.172720</td>\n",
       "      <td>5.691814</td>\n",
       "      <td>0.868742</td>\n",
       "      <td>0.871618</td>\n",
       "      <td>1.019680</td>\n",
       "      <td>1.343463</td>\n",
       "      <td>1.715302</td>\n",
       "      <td>1.478053</td>\n",
       "      <td>1.826775</td>\n",
       "      <td>0.575467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>start1.csv</td>\n",
       "      <td>LL_LA</td>\n",
       "      <td>79.725996</td>\n",
       "      <td>752.646998</td>\n",
       "      <td>59.213480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.247032</td>\n",
       "      <td>5.612866</td>\n",
       "      <td>0.870433</td>\n",
       "      <td>0.624949</td>\n",
       "      <td>0.968962</td>\n",
       "      <td>1.261006</td>\n",
       "      <td>1.672951</td>\n",
       "      <td>1.480150</td>\n",
       "      <td>1.823616</td>\n",
       "      <td>0.575467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>start1.csv</td>\n",
       "      <td>LL_RA</td>\n",
       "      <td>79.726315</td>\n",
       "      <td>752.646998</td>\n",
       "      <td>59.228939</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.187843</td>\n",
       "      <td>5.675430</td>\n",
       "      <td>0.868395</td>\n",
       "      <td>0.985380</td>\n",
       "      <td>0.974733</td>\n",
       "      <td>1.220224</td>\n",
       "      <td>1.736408</td>\n",
       "      <td>1.483196</td>\n",
       "      <td>1.824611</td>\n",
       "      <td>0.575467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>start1.csv</td>\n",
       "      <td>Vx_RL</td>\n",
       "      <td>79.727565</td>\n",
       "      <td>752.646998</td>\n",
       "      <td>59.149022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120591</td>\n",
       "      <td>5.612866</td>\n",
       "      <td>0.864282</td>\n",
       "      <td>0.902979</td>\n",
       "      <td>0.941243</td>\n",
       "      <td>1.210625</td>\n",
       "      <td>1.714994</td>\n",
       "      <td>1.473518</td>\n",
       "      <td>1.819468</td>\n",
       "      <td>0.575467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 85 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3-1-2. PPG HR_HRV\n",
    "---\n",
    "### Function PPG_metrics\n",
    "\n",
    "##### - 한 Subject의 모든 PPG 데이터를 분석하여 HR, HRV 지표를 계산하고 결과를 반환.\n",
    "- Args:\n",
    "    - subject_PPG_dir_path (str): 분석할 Subject의 ECG 데이터가 담긴 상위 디렉토리 경로. (e.g: '.../data/sub1/PPG')\n",
    "    - save_path (str, optional): 분석 결과(CSV, plot)를 저장할 최상위 디렉토리 경로.\n",
    "    - show (bool, optional): 분석 과정에서 생성되는 plot을 화면에 표시할지 여부. save_path가 지정된 경우에만 활성화.\n",
    "    - unit (str, optional): PPG 신호의 단위 (예: 'mV'). Plot 제목에 사용.\n",
    "- Returns:\n",
    "    - pd.DataFrame: Subject의 모든 파일과 채널에 대한 HR, HRV 분석 결과가 포함된 데이터프레임.\n",
    "\n",
    "### Function save_PPG_HR_plot\n",
    "##### - PPG 신호를 분석하여 평균 심박수(HR)를 계산하고, 관련 Plot 저장.\n",
    "- Args:\n",
    "    - ppg_signal (np.ndarray): 분석할 Raw PPG 신호.\n",
    "    - show (bool, optional): 분석 Plot을 화면에 표시할지 여부.\n",
    "    - title (str, optional): Plot에 표시될 제목.\n",
    "    - save_path (str, optional): Plot을 저장할 경로 및 파일명.\n",
    "    - unit (str, optional): 신호의 단위 (y축 라벨에 사용).\n",
    "- Returns:\n",
    "    - float: 계산된 평균 심박수(PPG_Rate_Mean). 오류 발생 시 0을 반환.\n",
    "\n",
    "### Function save_ECG_HRV_plot\n",
    "##### - PPG 신호에서 심박 변이도(HRV) 지표들을 계산하고, 관련 Plot 저장.\n",
    "- Args:\n",
    "    - ppg_signal (np.ndarray): 분석할 Raw PPG 신호.\n",
    "    - show (bool, optional): 분석 Plot을 화면에 표시할지 여부.\n",
    "    - title (str, optional): Plot에 표시될 제목.\n",
    "    - save_path (str, optional): Plot을 저장할 경로 및 파일명.\n",
    "- Returns:\n",
    "    - pd.DataFrame | None: 계산된 모든 HRV 지표가 포함된 DataFrame. 오류 발생 시 None을 반환."
   ],
   "id": "cb0ddafab8ddac9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:58:42.765787Z",
     "start_time": "2025-09-30T09:58:42.753011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def PPG_metrics(subject_PPG_dir_path, save_path=None, show=False, unit=None):\n",
    "    \"\"\"\n",
    "    한 Subject의 모든 PPG 파일을 분석하여 HR, HRV 지표를 계산하고 결과를 반환.\n",
    "\n",
    "    Args:\n",
    "        subject_PPG_dir_path (str): 분석할 Subject의 PPG 데이터가 담긴 디렉토리 경로.(예: '.../data/sub1/PPG')\n",
    "        save_path (str, optional): 분석 결과(CSV, plot)를 저장할 최상위 디렉토리 경로.\n",
    "        show (bool, optional): 분석 과정에서 생성되는 plot을 화면에 표시할지 여부. save_path가 지정된 경우에만 활성화.\n",
    "        unit (str, optional): PPG 신호의 단위. Plot에 사용.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            Subject의 모든 PPG 파일에 대한 HR, HRV 분석 결과가 포함된 데이터프레임.\n",
    "    \"\"\"\n",
    "    print(subject_PPG_dir_path+ ' PPG Signal - HR, HRV 분석 시작')\n",
    "    # 최종 결과를 저장하기 위한 빈 데이터프레임 생성\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # subject 내 PPG 디렉토리 파일들 순회 (low, mid, high)\n",
    "    for dir_name in os.listdir(subject_PPG_dir_path):\n",
    "        dir_path = os.path.join(subject_PPG_dir_path, dir_name)\n",
    "\n",
    "        # 데이터 저장을 위한 Subject 이름 추출\n",
    "        subject_path = subject_PPG_dir_path.split('\\\\')[3]\n",
    "        # 시각화를 원한다면 시각화 결과를 저장할 폴더 생성\n",
    "        if save_path and show:\n",
    "            os.makedirs(save_path + '/' + subject_path, exist_ok=True)\n",
    "\n",
    "        # PPG 데이터 파일 순회 (low1.csv, low2.csv, ...)\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            file_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "            # 파일명에서 확장자를 제외한 부분 추출 (e.g., 'low1')\n",
    "            title_name = file_name.split('.')[0]\n",
    "            try:\n",
    "\n",
    "                # CSV 파일에서 PPG 신호 추출\n",
    "                raw_df = pd.read_csv(file_path)\n",
    "                raw_signal = extract_signal(raw_df, 'PPG')\n",
    "                raw = raw_signal['ppg']\n",
    "\n",
    "                # 한 파일의 분석 결과를 임시로 담을 데이터프레임 생성\n",
    "                temp = pd.DataFrame()\n",
    "                temp[\"File\"] = [file_name]\n",
    "                temp[\"channel\"] = \"ppg\"\n",
    "\n",
    "                title = '' + title_name + ' - PPG '\n",
    "\n",
    "                # HR(심박수) 계산 및 Plot 저장\n",
    "                HR = save_PPG_HR_plot(raw, show, title=title,\n",
    "                                      save_path=save_path + '/' + subject_path + '/' + title_name + ' - PPG ' if save_path and show else None,\n",
    "                                      unit=unit)\n",
    "                temp[\"HR\"] = HR\n",
    "\n",
    "                # HRV(심박 변이률) 계산 및 Plot 저장\n",
    "                HRV = save_PPG_HRV_plot(raw, show, title=title,\n",
    "                                        save_path=save_path + '/' + subject_path + '/' + title_name + ' - PPG ' if save_path and show else None)\n",
    "\n",
    "                # HR과 HRV 결과를 임시 데이터프레임에 병합\n",
    "                result = pd.concat([temp, HRV], axis=1)\n",
    "\n",
    "                # 최종 데이터프레임에 현재 채널의 결과 누적\n",
    "                df = pd.concat([df, result], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file_path}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "    # 최종 결과를 CSV 파일로 저장 (save_path가 지정된 경우)\n",
    "    if save_path:\n",
    "        # 저장 경로가 없으면 생성\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        # CSV로 저장\n",
    "        save_file = os.path.join(save_path, subject_PPG_dir_path.split('\\\\')[-2] + '.csv')\n",
    "        df.to_csv(save_file, index=False)\n",
    "\n",
    "        # 파일 저장 완료 메시지 출력\n",
    "        print(f\"{save_file}에 PPG Signal - HR, HRV 분석 결과를 저장했습니다.\")\n",
    "    # 분석 결과가 담긴 최종 데이터프레임 반환\n",
    "    return df\n",
    "\n",
    "def save_PPG_HR_plot(ppg_signal, show=False, title=None, save_path=None, unit=None):\n",
    "    \"\"\"\n",
    "    PPG 신호를 분석하여 평균 심박수(HR)를 계산하고, 관련 Plot을 저장.\n",
    "\n",
    "    Args:\n",
    "        ppg_signal (np.ndarray): 분석할 Raw PPG 신호 배열.\n",
    "        show (bool, optional): 분석 Plot을 화면에 표시할지 여부.\n",
    "        title (str, optional): Plot에 표시될 제목.\n",
    "        save_path (str, optional): Plot을 저장할 경로 및 파일명.\n",
    "        unit (str, optional): 신호의 단위 (y축 라벨에 사용).\n",
    "\n",
    "    Returns:\n",
    "        float | None: 계산된 평균 심박수(PPG_Rate_Mean). 오류 발생 시 None을 반환.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PPG 신호 처리 및 분석\n",
    "        signals, info = nk.ppg_process(ppg_signal, sampling_rate=FS_PPG)\n",
    "        analyze_df = nk.ppg_analyze(signals, sampling_rate=FS_PPG)\n",
    "\n",
    "        # save_path가 제공되면 PPG 처리 과정 Plot 저장\n",
    "        if save_path:\n",
    "            nk.ppg_plot(signals, info, show=show, title=title + ' (HR)', save_path=save_path + ' (HR)', unit=unit)\n",
    "\n",
    "        # 분석 결과에서 평균 심박수(HR) 값 추출\n",
    "        mean_hr = analyze_df['PPG_Rate_Mean'].values[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR - get_PPG_HR] {e}\")\n",
    "        mean_hr = None\n",
    "\n",
    "    return mean_hr\n",
    "\n",
    "\n",
    "def save_PPG_HRV_plot(ppg_signal, show=False, title=None, save_path=None):\n",
    "    \"\"\"\n",
    "    PPG 신호에서 심박 변이도(HRV) 지표들을 계산하고, 관련 Plot을 저장.\n",
    "\n",
    "    Args:\n",
    "        ppg_signal (np.ndarray): 분석할 Raw PPG 신호 배열.\n",
    "        show (bool, optional): 분석 Plot을 화면에 표시할지 여부.\n",
    "        title (str, optional): Plot에 표시될 제목.\n",
    "        save_path (str, optional): Plot을 저장할 경로 및 파일명.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            계산된 모든 HRV 지표가 포함된 DataFrame. 오류 발생 시 빈 DataFrame을 반환.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # PPG 신호 처리 및 Peak 검출\n",
    "        processed, info = nk.ppg_process(ppg_signal, sampling_rate=FS_PPG)\n",
    "        peaks = info.get(\"PPG_Peaks\")\n",
    "\n",
    "        # 검출된 Peak를 기반으로 HRV 지표 계산 및 시각화\n",
    "        hrv = nk.hrv(peaks, sampling_rate=FS_PPG, show=show, title=title + ' (HRV)' if title else None, save_path=save_path + ' (HRV)' if save_path else None)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "        return hrv\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[get_PPG_HRV ERROR] {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()"
   ],
   "id": "30c906ed43ddc81f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:58:48.030438Z",
     "start_time": "2025-09-30T09:58:42.859907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_data_dir = r\"..\\data\\split_data\"\n",
    "save_path = \"..\\\\features\\\\HR_HRV\\\\PPG\\\\\"+subject\n",
    "# Save\n",
    "PPG_HR_HRV_result = PPG_metrics(os.path.join(split_data_dir, subject)+\"\\\\PPG\", save_path=save_path)\n",
    "PPG_HR_HRV_result"
   ],
   "id": "3a310f987eedf88b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\data\\split_data\\1_1_011_V2\\PPG PPG Signal - HR, HRV 분석 시작\n",
      "..\\features\\HR_HRV\\PPG\\1_1_011_V2\\1_1_011_V2.csv에 PPG Signal - HR, HRV 분석 결과를 저장했습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "          File channel         HR  HRV_MeanNN   HRV_SDNN  HRV_SDANN1  \\\n",
       "0     end1.csv     ppg  78.348128  766.145833  30.219041         NaN   \n",
       "1    high1.csv     ppg  74.778557  802.382172  32.544612         NaN   \n",
       "2    high2.csv     ppg  75.061770  799.457097  33.607610         NaN   \n",
       "3    high3.csv     ppg  72.514633  827.256944  31.204344         NaN   \n",
       "4    high4.csv     ppg  73.943779  811.567164  34.217052         NaN   \n",
       "5    high5.csv     ppg  71.299928  841.557018  37.329130         NaN   \n",
       "6    high6.csv     ppg  75.481918  795.200893  22.116327         NaN   \n",
       "7    high7.csv     ppg  74.436819  806.189904  38.987307         NaN   \n",
       "8     low1.csv     ppg  75.823003  791.638963  26.029619         NaN   \n",
       "9     low2.csv     ppg  73.767095  813.419118  29.959570         NaN   \n",
       "10    low3.csv     ppg  71.486583  839.409722  28.996115         NaN   \n",
       "11    low4.csv     ppg  71.236390  842.391304  35.326722         NaN   \n",
       "12    low5.csv     ppg  76.293397  786.437988  37.519466         NaN   \n",
       "13    low6.csv     ppg  72.312269  831.054688  47.626439         NaN   \n",
       "14    mid1.csv     ppg  73.810695  812.988281  28.666761         NaN   \n",
       "15    mid2.csv     ppg  74.520240  805.338542  28.850247         NaN   \n",
       "16    mid3.csv     ppg  71.443748  840.126812  34.566118         NaN   \n",
       "17    mid4.csv     ppg  74.765866  802.556818  25.843628         NaN   \n",
       "18    mid5.csv     ppg  73.175184  819.905599  27.173907         NaN   \n",
       "19  start1.csv     ppg  79.594955  753.855519  60.553485         NaN   \n",
       "\n",
       "    HRV_SDNNI1  HRV_SDANN2  HRV_SDNNI2  HRV_SDANN5  ...  HRV_SampEn  \\\n",
       "0          NaN         NaN         NaN         NaN  ...    1.746909   \n",
       "1          NaN         NaN         NaN         NaN  ...    1.536235   \n",
       "2          NaN         NaN         NaN         NaN  ...    1.276293   \n",
       "3          NaN         NaN         NaN         NaN  ...    1.417066   \n",
       "4          NaN         NaN         NaN         NaN  ...    2.014903   \n",
       "5          NaN         NaN         NaN         NaN  ...    1.148623   \n",
       "6          NaN         NaN         NaN         NaN  ...    1.763589   \n",
       "7          NaN         NaN         NaN         NaN  ...    1.540445   \n",
       "8          NaN         NaN         NaN         NaN  ...    1.763589   \n",
       "9          NaN         NaN         NaN         NaN  ...    1.704748   \n",
       "10         NaN         NaN         NaN         NaN  ...    1.704748   \n",
       "11         NaN         NaN         NaN         NaN  ...    1.349927   \n",
       "12         NaN         NaN         NaN         NaN  ...    2.054124   \n",
       "13         NaN         NaN         NaN         NaN  ...         inf   \n",
       "14         NaN         NaN         NaN         NaN  ...    1.148623   \n",
       "15         NaN         NaN         NaN         NaN  ...    1.218157   \n",
       "16         NaN         NaN         NaN         NaN  ...    2.047693   \n",
       "17         NaN         NaN         NaN         NaN  ...    1.304949   \n",
       "18         NaN         NaN         NaN         NaN  ...    1.969441   \n",
       "19         NaN         NaN         NaN         NaN  ...    1.473306   \n",
       "\n",
       "    HRV_ShanEn  HRV_FuzzyEn  HRV_MSEn  HRV_CMSEn  HRV_RCMSEn    HRV_CD  \\\n",
       "0     2.581590     1.409175  1.205974   0.995721    1.100123  0.871467   \n",
       "1     2.603172     1.262478  1.151919   0.893195    0.882681  0.823186   \n",
       "2     2.738377     1.451623  0.593726   0.871824    0.905863  0.814910   \n",
       "3     2.571013     1.567166  0.000000   0.866060    0.874127  0.629120   \n",
       "4     2.749999     1.484751  0.951666   1.368390    1.350196  1.051886   \n",
       "5     2.830006     1.191028  0.000000   0.955598    1.318366  0.844857   \n",
       "6     2.178657     1.991092  0.843257   0.770779    0.765718  0.372395   \n",
       "7     2.961880     1.528761  0.834363   1.153326    1.711985  1.068179   \n",
       "8     2.375179     1.589706  0.715550   0.798672    0.802627  0.509914   \n",
       "9     2.583877     1.532479  0.000000   0.912665    1.103200  0.882456   \n",
       "10    2.515644     1.452570  0.000000   0.946047    1.025661  0.612828   \n",
       "11    2.685361     1.568669  0.000000   0.566554    0.684055  0.889195   \n",
       "12    2.867353     1.224807  0.961471   0.000000    0.000000  0.997063   \n",
       "13    3.096039     1.352422       NaN        NaN         NaN  1.393116   \n",
       "14    2.469361     1.448901  0.000000   1.351811    1.421064  0.735353   \n",
       "15    2.504109     1.346240  0.948775   1.140429    1.227157  0.788194   \n",
       "16    2.770327     1.552834  0.914283   0.953296    0.921212  0.953063   \n",
       "17    2.383362     1.461138  0.000000   0.916130    1.075493  0.555564   \n",
       "18    2.429463     1.573910  0.000000   1.133142    1.100299  0.721965   \n",
       "19    3.435537     0.932217  1.277145   0.877310    1.450431  1.342400   \n",
       "\n",
       "     HRV_HFD   HRV_KFD   HRV_LZC  \n",
       "0   1.828831  3.258405  1.079662  \n",
       "1   1.786617  2.623491  1.069477  \n",
       "2   1.978206  2.519035  0.897352  \n",
       "3   2.013709  3.026159  0.976329  \n",
       "4   1.866853  3.224590  1.177002  \n",
       "5   1.716691  2.756331  0.716320  \n",
       "6   2.031563  4.410982  1.283885  \n",
       "7   1.783813  3.544902  0.833866  \n",
       "8   1.815162  3.846226  1.181827  \n",
       "9   1.945821  3.340232  1.223464  \n",
       "10  1.836753  3.462374  1.098371  \n",
       "11  1.846766  2.971787  0.960619  \n",
       "12  1.858016  2.321110  0.750000  \n",
       "13  1.752797  2.009620  0.931337  \n",
       "14  1.832119  3.132471  1.279887  \n",
       "15  1.853970  4.748713  1.082930  \n",
       "16  1.870941  2.491180  0.885293  \n",
       "17  1.914879  3.195932  1.156272  \n",
       "18  1.953146  4.419247  1.163534  \n",
       "19  1.525501  2.076785  0.569708  \n",
       "\n",
       "[20 rows x 85 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>channel</th>\n",
       "      <th>HR</th>\n",
       "      <th>HRV_MeanNN</th>\n",
       "      <th>HRV_SDNN</th>\n",
       "      <th>HRV_SDANN1</th>\n",
       "      <th>HRV_SDNNI1</th>\n",
       "      <th>HRV_SDANN2</th>\n",
       "      <th>HRV_SDNNI2</th>\n",
       "      <th>HRV_SDANN5</th>\n",
       "      <th>...</th>\n",
       "      <th>HRV_SampEn</th>\n",
       "      <th>HRV_ShanEn</th>\n",
       "      <th>HRV_FuzzyEn</th>\n",
       "      <th>HRV_MSEn</th>\n",
       "      <th>HRV_CMSEn</th>\n",
       "      <th>HRV_RCMSEn</th>\n",
       "      <th>HRV_CD</th>\n",
       "      <th>HRV_HFD</th>\n",
       "      <th>HRV_KFD</th>\n",
       "      <th>HRV_LZC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end1.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>78.348128</td>\n",
       "      <td>766.145833</td>\n",
       "      <td>30.219041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.746909</td>\n",
       "      <td>2.581590</td>\n",
       "      <td>1.409175</td>\n",
       "      <td>1.205974</td>\n",
       "      <td>0.995721</td>\n",
       "      <td>1.100123</td>\n",
       "      <td>0.871467</td>\n",
       "      <td>1.828831</td>\n",
       "      <td>3.258405</td>\n",
       "      <td>1.079662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high1.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>74.778557</td>\n",
       "      <td>802.382172</td>\n",
       "      <td>32.544612</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.536235</td>\n",
       "      <td>2.603172</td>\n",
       "      <td>1.262478</td>\n",
       "      <td>1.151919</td>\n",
       "      <td>0.893195</td>\n",
       "      <td>0.882681</td>\n",
       "      <td>0.823186</td>\n",
       "      <td>1.786617</td>\n",
       "      <td>2.623491</td>\n",
       "      <td>1.069477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high2.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>75.061770</td>\n",
       "      <td>799.457097</td>\n",
       "      <td>33.607610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.276293</td>\n",
       "      <td>2.738377</td>\n",
       "      <td>1.451623</td>\n",
       "      <td>0.593726</td>\n",
       "      <td>0.871824</td>\n",
       "      <td>0.905863</td>\n",
       "      <td>0.814910</td>\n",
       "      <td>1.978206</td>\n",
       "      <td>2.519035</td>\n",
       "      <td>0.897352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high3.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>72.514633</td>\n",
       "      <td>827.256944</td>\n",
       "      <td>31.204344</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.417066</td>\n",
       "      <td>2.571013</td>\n",
       "      <td>1.567166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.866060</td>\n",
       "      <td>0.874127</td>\n",
       "      <td>0.629120</td>\n",
       "      <td>2.013709</td>\n",
       "      <td>3.026159</td>\n",
       "      <td>0.976329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high4.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>73.943779</td>\n",
       "      <td>811.567164</td>\n",
       "      <td>34.217052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.014903</td>\n",
       "      <td>2.749999</td>\n",
       "      <td>1.484751</td>\n",
       "      <td>0.951666</td>\n",
       "      <td>1.368390</td>\n",
       "      <td>1.350196</td>\n",
       "      <td>1.051886</td>\n",
       "      <td>1.866853</td>\n",
       "      <td>3.224590</td>\n",
       "      <td>1.177002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>high5.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>71.299928</td>\n",
       "      <td>841.557018</td>\n",
       "      <td>37.329130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148623</td>\n",
       "      <td>2.830006</td>\n",
       "      <td>1.191028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.955598</td>\n",
       "      <td>1.318366</td>\n",
       "      <td>0.844857</td>\n",
       "      <td>1.716691</td>\n",
       "      <td>2.756331</td>\n",
       "      <td>0.716320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>high6.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>75.481918</td>\n",
       "      <td>795.200893</td>\n",
       "      <td>22.116327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.763589</td>\n",
       "      <td>2.178657</td>\n",
       "      <td>1.991092</td>\n",
       "      <td>0.843257</td>\n",
       "      <td>0.770779</td>\n",
       "      <td>0.765718</td>\n",
       "      <td>0.372395</td>\n",
       "      <td>2.031563</td>\n",
       "      <td>4.410982</td>\n",
       "      <td>1.283885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>high7.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>74.436819</td>\n",
       "      <td>806.189904</td>\n",
       "      <td>38.987307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.540445</td>\n",
       "      <td>2.961880</td>\n",
       "      <td>1.528761</td>\n",
       "      <td>0.834363</td>\n",
       "      <td>1.153326</td>\n",
       "      <td>1.711985</td>\n",
       "      <td>1.068179</td>\n",
       "      <td>1.783813</td>\n",
       "      <td>3.544902</td>\n",
       "      <td>0.833866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>low1.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>75.823003</td>\n",
       "      <td>791.638963</td>\n",
       "      <td>26.029619</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.763589</td>\n",
       "      <td>2.375179</td>\n",
       "      <td>1.589706</td>\n",
       "      <td>0.715550</td>\n",
       "      <td>0.798672</td>\n",
       "      <td>0.802627</td>\n",
       "      <td>0.509914</td>\n",
       "      <td>1.815162</td>\n",
       "      <td>3.846226</td>\n",
       "      <td>1.181827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>low2.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>73.767095</td>\n",
       "      <td>813.419118</td>\n",
       "      <td>29.959570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.704748</td>\n",
       "      <td>2.583877</td>\n",
       "      <td>1.532479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.912665</td>\n",
       "      <td>1.103200</td>\n",
       "      <td>0.882456</td>\n",
       "      <td>1.945821</td>\n",
       "      <td>3.340232</td>\n",
       "      <td>1.223464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>low3.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>71.486583</td>\n",
       "      <td>839.409722</td>\n",
       "      <td>28.996115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.704748</td>\n",
       "      <td>2.515644</td>\n",
       "      <td>1.452570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946047</td>\n",
       "      <td>1.025661</td>\n",
       "      <td>0.612828</td>\n",
       "      <td>1.836753</td>\n",
       "      <td>3.462374</td>\n",
       "      <td>1.098371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>low4.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>71.236390</td>\n",
       "      <td>842.391304</td>\n",
       "      <td>35.326722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.349927</td>\n",
       "      <td>2.685361</td>\n",
       "      <td>1.568669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566554</td>\n",
       "      <td>0.684055</td>\n",
       "      <td>0.889195</td>\n",
       "      <td>1.846766</td>\n",
       "      <td>2.971787</td>\n",
       "      <td>0.960619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>low5.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>76.293397</td>\n",
       "      <td>786.437988</td>\n",
       "      <td>37.519466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.054124</td>\n",
       "      <td>2.867353</td>\n",
       "      <td>1.224807</td>\n",
       "      <td>0.961471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997063</td>\n",
       "      <td>1.858016</td>\n",
       "      <td>2.321110</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>low6.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>72.312269</td>\n",
       "      <td>831.054688</td>\n",
       "      <td>47.626439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>inf</td>\n",
       "      <td>3.096039</td>\n",
       "      <td>1.352422</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.393116</td>\n",
       "      <td>1.752797</td>\n",
       "      <td>2.009620</td>\n",
       "      <td>0.931337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mid1.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>73.810695</td>\n",
       "      <td>812.988281</td>\n",
       "      <td>28.666761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148623</td>\n",
       "      <td>2.469361</td>\n",
       "      <td>1.448901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.351811</td>\n",
       "      <td>1.421064</td>\n",
       "      <td>0.735353</td>\n",
       "      <td>1.832119</td>\n",
       "      <td>3.132471</td>\n",
       "      <td>1.279887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mid2.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>74.520240</td>\n",
       "      <td>805.338542</td>\n",
       "      <td>28.850247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.218157</td>\n",
       "      <td>2.504109</td>\n",
       "      <td>1.346240</td>\n",
       "      <td>0.948775</td>\n",
       "      <td>1.140429</td>\n",
       "      <td>1.227157</td>\n",
       "      <td>0.788194</td>\n",
       "      <td>1.853970</td>\n",
       "      <td>4.748713</td>\n",
       "      <td>1.082930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mid3.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>71.443748</td>\n",
       "      <td>840.126812</td>\n",
       "      <td>34.566118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.047693</td>\n",
       "      <td>2.770327</td>\n",
       "      <td>1.552834</td>\n",
       "      <td>0.914283</td>\n",
       "      <td>0.953296</td>\n",
       "      <td>0.921212</td>\n",
       "      <td>0.953063</td>\n",
       "      <td>1.870941</td>\n",
       "      <td>2.491180</td>\n",
       "      <td>0.885293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mid4.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>74.765866</td>\n",
       "      <td>802.556818</td>\n",
       "      <td>25.843628</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.304949</td>\n",
       "      <td>2.383362</td>\n",
       "      <td>1.461138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.916130</td>\n",
       "      <td>1.075493</td>\n",
       "      <td>0.555564</td>\n",
       "      <td>1.914879</td>\n",
       "      <td>3.195932</td>\n",
       "      <td>1.156272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mid5.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>73.175184</td>\n",
       "      <td>819.905599</td>\n",
       "      <td>27.173907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.969441</td>\n",
       "      <td>2.429463</td>\n",
       "      <td>1.573910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.133142</td>\n",
       "      <td>1.100299</td>\n",
       "      <td>0.721965</td>\n",
       "      <td>1.953146</td>\n",
       "      <td>4.419247</td>\n",
       "      <td>1.163534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>start1.csv</td>\n",
       "      <td>ppg</td>\n",
       "      <td>79.594955</td>\n",
       "      <td>753.855519</td>\n",
       "      <td>60.553485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.473306</td>\n",
       "      <td>3.435537</td>\n",
       "      <td>0.932217</td>\n",
       "      <td>1.277145</td>\n",
       "      <td>0.877310</td>\n",
       "      <td>1.450431</td>\n",
       "      <td>1.342400</td>\n",
       "      <td>1.525501</td>\n",
       "      <td>2.076785</td>\n",
       "      <td>0.569708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 85 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3-2. PTT Calculate\n",
    "---\n",
    "### Function calculate_ptt_peak\n",
    "##### - ECG와 PPG 신호로부터 PTT(Pulse Transit Time)를 계산.\n",
    "- Args:\n",
    "    - ecg_file (str): ECG 데이터 CSV 파일의 경로.\n",
    "    - ppg_file (str): PPG 데이터 CSV 파일의 경로.\n",
    "    - ecg_col (str): ECG 신호가 저장된 컬럼명.\n",
    "    - ppg_col (str): PPG 신호가 저장된 컬럼명.\n",
    "    - ptt_range (tuple[int, int], optional): 유효한 PTT 값의 범위(ms).\n",
    "- Returns:\n",
    "    - dict | None:\n",
    "        - PTT 분석 결과(값 리스트, 평균, 표준편차)가 담긴 딕셔너리.\n",
    "        - 데이터 로딩 또는 처리 중 오류 발생 시 None을 반환.\n",
    "\n",
    "### Function process_subject_PTT\n",
    "##### - 한 Subject의 모든 세션/파일에 대해 PTT를 계산하고 결과를 DataFrame으로 통합.\n",
    "- Args:\n",
    "    - base_dir (str): 'ECG'와 'PPG' 폴더를 포함하는 Subject 데이터의 경로.\n",
    "    - subject (str): 현재 처리 중인 Subject의 ID (결과 저장용).\n",
    "    - save_dir (str): PTT를 저장할 디렉토리\n",
    "- Returns:\n",
    "    - pd.DataFrame:\n",
    "        Subject의 모든 파일과 ECG 채널별 PTT 분석 결과가 포함된 데이터프레임."
   ],
   "id": "ee92992c5f116b8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:58:48.431809Z",
     "start_time": "2025-09-30T09:58:48.417252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_ptt_peak(ecg_file, ppg_file, ecg_col, ppg_col, ptt_range=(150, 450)):\n",
    "    \"\"\"\n",
    "    ECG와 PPG 신호로부터 PTT(Pulse Transit Time)를 계산.\n",
    "\n",
    "    Args:\n",
    "        ecg_file (str): ECG 데이터 CSV 파일의 경로.\n",
    "        ppg_file (str): PPG 데이터 CSV 파일의 경로.\n",
    "        ecg_col (str): ECG 신호가 저장된 컬럼명.\n",
    "        ppg_col (str): PPG 신호가 저장된 컬럼명.\n",
    "        ptt_range (tuple[int, int], optional): 유효한 PTT 값의 범위(ms).\n",
    "\n",
    "    Returns:\n",
    "        dict | None:\n",
    "            PTT 분석 결과(값 리스트, 평균, 표준편차)가 담긴 딕셔너리.\n",
    "            데이터 로딩 또는 처리 중 오류 발생 시 None을 반환합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 데이터 로드 및 동기화\n",
    "        # CSV 파일 로드\n",
    "        df_ecg = pd.read_csv(ecg_file)\n",
    "        df_ppg = pd.read_csv(ppg_file)\n",
    "\n",
    "        # UNIX 타임스탬프를 datetime 객체로 변환하고 신호 전처리\n",
    "        df_ecg['Timestamp'] = pd.to_datetime(df_ecg['Shimmer_820D_Timestamp_Unix_CAL'], unit='ms')\n",
    "        df_ecg[ecg_col] = nk.ecg_clean(df_ecg[ecg_col], sampling_rate=512)\n",
    "        df_ppg['Timestamp'] = pd.to_datetime(df_ppg['id95AE_Timestamp_Unix_CAL'], unit='ms')\n",
    "        df_ppg[ppg_col] = nk.ppg_clean(df_ppg[ppg_col], sampling_rate=51.2)\n",
    "\n",
    "        # 타임스탬프를 인덱스로 설정하여 두 데이터프레임 병합\n",
    "        df_ecg = df_ecg.set_index('Timestamp')\n",
    "        df_ppg = df_ppg.set_index('Timestamp')\n",
    "        df = pd.concat([df_ecg[ecg_col], df_ppg[ppg_col]], axis=1)\n",
    "\n",
    "         # 샘플링 레이트가 다른 ppg 신호를 보간(interpolate)하여 맞춤\n",
    "        df[ppg_col] = df[ppg_col].interpolate(method='cubic')\n",
    "\n",
    "        # 동기화 후 결측치가 있는 행 제거\n",
    "        df = df.dropna(subset=[ecg_col, ppg_col])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"데이터 로딩 오류: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 동기화된 데이터의 타임스탬프 간격 중앙값을 이용해 실제 샘플링 레이트 계산\n",
    "    time_diff = df.index.to_series().diff().median().total_seconds()\n",
    "    sampling_rate = 1 / time_diff\n",
    "\n",
    "    # --- ECG R-peak 탐지 ---\n",
    "    signals, info = nk.ecg_process(df[ecg_col], sampling_rate=sampling_rate)\n",
    "    ecg_peaks = info['ECG_R_Peaks']\n",
    "    ecg_peak_times = df.index[ecg_peaks] # R-peak 발생 시간\n",
    "\n",
    "    # --- PPG 처리 ---\n",
    "    signals, info = nk.ppg_process(df[ppg_col], sampling_rate=sampling_rate)\n",
    "    ppg_peaks = info[\"PPG_Peaks\"]\n",
    "    ppg_peak_times = df.index[ppg_peaks] # PPG peak 발생 시간\n",
    "\n",
    "    # --- PTT 계산 (R-peak → PPG peak) ---\n",
    "    ptt_peak = []\n",
    "    for ecg_time in ecg_peak_times:\n",
    "        # 현재 R-peak 이후에 발생한 PPG peak들만 필터링\n",
    "        future_peaks = ppg_peak_times[ppg_peak_times > ecg_time]\n",
    "        if not future_peaks.empty:\n",
    "            # 가장 먼저 나타나는 PPG peak와의 시간 차이(ms) 계산\n",
    "            dt = (future_peaks[0] - ecg_time).total_seconds() * 1000\n",
    "            # 계산된 PTT가 유효한 범위 내에 있는지 확인 후 추가\n",
    "            if ptt_range[0] < dt < ptt_range[1]:\n",
    "                ptt_peak.append(dt)\n",
    "\n",
    "    # 결과 정리\n",
    "    results = {\n",
    "        \"ptt_peak_values\": ptt_peak,\n",
    "        \"ptt_peak_mean\": np.mean(ptt_peak) if len(ptt_peak) > 0 else None,\n",
    "        \"ptt_peak_std\": np.std(ptt_peak) if len(ptt_peak) > 0 else None,\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_subject_PTT(base_dir, subject, save_path = None):\n",
    "    \"\"\"\n",
    "    한 Subject의 모든 세션/파일에 대해 PTT를 계산하고 결과를 DataFrame으로 통합.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): 'ECG'와 'PPG' 폴더를 포함하는 Subject 데이터의 경로.\n",
    "        subject (str): 현재 처리 중인 Subject의 ID (결과 저장용).\n",
    "        save_dir (str): PTT를 저장할 디렉토리\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            Subject의 모든 파일과 ECG 채널별 PTT 분석 결과가 포함된 데이터프레임.\n",
    "    \"\"\"\n",
    "    print(base_dir+' PTT 분석 시작')\n",
    "    final_data = []\n",
    "\n",
    "    # 분석할 ECG/PPG 채널명 정의\n",
    "    ecg_channels = [\n",
    "        'Shimmer_820D_ECG_LA-RA_24BIT_CAL',\n",
    "        'Shimmer_820D_ECG_LL-LA_24BIT_CAL',\n",
    "        'Shimmer_820D_ECG_LL-RA_24BIT_CAL',\n",
    "        'Shimmer_820D_ECG_Vx-RL_24BIT_CAL'\n",
    "    ]\n",
    "    ppg_channel = 'id95AE_PPG_A13_CAL'\n",
    "\n",
    "    # ECG 및 PPG 데이터의 기본 경로 설정\n",
    "    ECG_path = os.path.join(base_dir, 'ECG')\n",
    "    PPG_path = os.path.join(base_dir, 'PPG')\n",
    "\n",
    "    # 각 디렉토리 파일 순회 (low1.csv, low2.csv, ...)\n",
    "    for condition_dir in ['start', 'low', 'mid', 'high', 'end']:\n",
    "        ECG_sub_path = os.path.join(ECG_path, condition_dir)\n",
    "        PPG_sub_path = os.path.join(PPG_path, condition_dir)\n",
    "\n",
    "        # 각 조건(start, low, mid, high, end) 폴더 내의 파일 순회\n",
    "        for file_name in os.listdir(ECG_sub_path):\n",
    "            ecg_file = os.path.join(ECG_sub_path, file_name)\n",
    "            ppg_file = os.path.join(PPG_sub_path, file_name)\n",
    "\n",
    "            # 대응하는 ECG와 PPG 파일이 모두 존재하는지 확인\n",
    "            if os.path.exists(ecg_file) and os.path.exists(ppg_file):\n",
    "                # 한 파일의 결과를 저장할 딕셔너리 초기화\n",
    "                row_data = {\n",
    "                    'Subject': subject,\n",
    "                    # 'Condition': condition_dir,\n",
    "                    'File': file_name\n",
    "                }\n",
    "                # 정의된 모든 ECG 채널에 대해 PTT 계산 반복\n",
    "                for ecg_channel in ecg_channels:\n",
    "                    try:\n",
    "                        # PTT 계산 함수 호출\n",
    "                        results = calculate_ptt_peak(ecg_file, ppg_file, ecg_channel, ppg_channel)\n",
    "                    except Exception as e:\n",
    "                        print(f\"오류 발생 (파일: {file_name}, 채널: {ecg_channel}): {e}\")\n",
    "                        # 오류 시 결과값 None으로 채우기\n",
    "                        row_data[f\"{ecg_channel.split('_')[-3]}_PTT_avg\"] = None\n",
    "                        row_data[f\"{ecg_channel.split('_')[-3]}_PTT_std\"] = None\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        # PTT 평균\n",
    "                        row_data[f\"{ecg_channel.split('_')[-3]}_PTT_avg\"] = results['ptt_peak_mean']\n",
    "                        # PTT 표준편차\n",
    "                        row_data[f\"{ecg_channel.split('_')[-3]}_PTT_std\"] = results['ptt_peak_std']\n",
    "                    except Exception as e :\n",
    "                        print(file_name, ecg_channel, e)\n",
    "                        row_data[f\"{ecg_channel.split('_')[-3]}_PTT_avg\"] = None\n",
    "                        row_data[f\"{ecg_channel.split('_')[-3]}_PTT_std\"] = None\n",
    "                        continue\n",
    "            # 한 파일에 대한 모든 채널의 분석 결과를 최종 리스트에 추가\n",
    "            final_data.append(row_data)\n",
    "    # 모든 파일에 대한 분석 결과 dataframe 변환\n",
    "    result = pd.DataFrame(final_data)\n",
    "\n",
    "    # save_path가 있으면 dataframe 저장\n",
    "    if save_path:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        print(os.path.join(save_dir, subject+\".csv\")+'에 PTT 계산 결과를 저장했습니다.')\n",
    "        result.to_csv(os.path.join(save_dir, subject+\".csv\"))\n",
    "    return result"
   ],
   "id": "367c5a6a09eebac7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:59:47.846301Z",
     "start_time": "2025-09-30T09:58:48.618388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_dir = r\"../features/PTT\"\n",
    "split_data_dir = r\"../data/split_data\"\n",
    "\n",
    "PTT_result = process_subject_PTT(os.path.join(split_data_dir, subject), subject, save_path=save_dir)\n",
    "PTT_result"
   ],
   "id": "883446e55c577d33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/split_data\\1_1_011_V2 PTT 분석 시작\n",
      "../features/PTT\\1_1_011_V2.csv에 PTT 계산 결과를 저장했습니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       Subject        File  LA-RA_PTT_avg  LA-RA_PTT_std  LL-LA_PTT_avg  \\\n",
       "0   1_1_011_V2  start1.csv     340.325234       4.878096     319.018390   \n",
       "1   1_1_011_V2    low1.csv     342.447458       2.743107     321.410563   \n",
       "2   1_1_011_V2    low2.csv     346.392216       2.560381     325.213804   \n",
       "3   1_1_011_V2    low3.csv     344.530933       8.825422     323.176378   \n",
       "4   1_1_011_V2    low4.csv     346.201532       6.674837     325.174021   \n",
       "5   1_1_011_V2    low5.csv     339.212169       3.197527     318.028446   \n",
       "6   1_1_011_V2    low6.csv     340.891293       3.860763     319.359268   \n",
       "7   1_1_011_V2    mid1.csv     342.569604       3.232687     320.881625   \n",
       "8   1_1_011_V2    mid2.csv     339.094550      19.991578     317.414883   \n",
       "9   1_1_011_V2    mid3.csv     348.108681       4.704282     326.701614   \n",
       "10  1_1_011_V2    mid4.csv     351.701679       5.984270     329.798589   \n",
       "11  1_1_011_V2    mid5.csv     361.088510       4.985522     339.524429   \n",
       "12  1_1_011_V2   high1.csv     365.296887       6.410581     343.781145   \n",
       "13  1_1_011_V2   high2.csv     346.894356       5.909033     325.211525   \n",
       "14  1_1_011_V2   high3.csv     340.864409      20.286770     319.690432   \n",
       "15  1_1_011_V2   high4.csv     343.577235       3.208363     322.150147   \n",
       "16  1_1_011_V2   high5.csv     347.958948       5.557481     326.777534   \n",
       "17  1_1_011_V2   high6.csv     345.798073       2.726569     324.504024   \n",
       "18  1_1_011_V2   high7.csv     351.532455       3.322364     330.137015   \n",
       "19  1_1_011_V2    end1.csv     350.482763       3.807185     328.946895   \n",
       "\n",
       "    LL-LA_PTT_std  LL-RA_PTT_avg  LL-RA_PTT_std  Vx-RL_PTT_avg  Vx-RL_PTT_std  \n",
       "0        4.546275     325.258260       4.557075     351.739584       4.660003  \n",
       "1        2.355311     327.636333       2.307750     353.962625       2.621523  \n",
       "2        2.267704     331.265078       2.190498     358.302216       2.425316  \n",
       "3        8.672358     329.383422       8.667981     356.888795       9.097955  \n",
       "4        6.691981     331.199787       6.543173     358.834426       6.628960  \n",
       "5        3.150460     324.638831       2.896865     351.652154       3.364131  \n",
       "6        3.979781     325.838024       3.831866     352.991073       3.977542  \n",
       "7        2.731930     327.392167       2.864705     354.898542       3.008923  \n",
       "8       20.154398     323.892667      20.235313     348.404383      30.908540  \n",
       "9        4.573676     333.063143       4.413494     360.676710       4.768780  \n",
       "10       6.492790     336.006911       6.413988     364.083036       6.028675  \n",
       "11       5.089271     345.941837       5.032136     373.564490       5.080556  \n",
       "12       6.501406     350.144403       6.306159     378.086790       6.520866  \n",
       "13       6.042743     331.567271       6.031639     359.142814       6.291320  \n",
       "14      20.343321     326.482273      20.467395     356.032489      15.093141  \n",
       "15       2.917700     328.555426       2.819510     356.042706       3.071168  \n",
       "16       4.503838     332.805379       5.378724     360.789034       5.215512  \n",
       "17       2.643870     330.554171       2.754741     358.135951       2.685678  \n",
       "18       3.188940     336.203364       3.150839     363.488106       3.209747  \n",
       "19       3.973402     335.166132       3.970868     362.150079       4.085384  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>File</th>\n",
       "      <th>LA-RA_PTT_avg</th>\n",
       "      <th>LA-RA_PTT_std</th>\n",
       "      <th>LL-LA_PTT_avg</th>\n",
       "      <th>LL-LA_PTT_std</th>\n",
       "      <th>LL-RA_PTT_avg</th>\n",
       "      <th>LL-RA_PTT_std</th>\n",
       "      <th>Vx-RL_PTT_avg</th>\n",
       "      <th>Vx-RL_PTT_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>start1.csv</td>\n",
       "      <td>340.325234</td>\n",
       "      <td>4.878096</td>\n",
       "      <td>319.018390</td>\n",
       "      <td>4.546275</td>\n",
       "      <td>325.258260</td>\n",
       "      <td>4.557075</td>\n",
       "      <td>351.739584</td>\n",
       "      <td>4.660003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>low1.csv</td>\n",
       "      <td>342.447458</td>\n",
       "      <td>2.743107</td>\n",
       "      <td>321.410563</td>\n",
       "      <td>2.355311</td>\n",
       "      <td>327.636333</td>\n",
       "      <td>2.307750</td>\n",
       "      <td>353.962625</td>\n",
       "      <td>2.621523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>low2.csv</td>\n",
       "      <td>346.392216</td>\n",
       "      <td>2.560381</td>\n",
       "      <td>325.213804</td>\n",
       "      <td>2.267704</td>\n",
       "      <td>331.265078</td>\n",
       "      <td>2.190498</td>\n",
       "      <td>358.302216</td>\n",
       "      <td>2.425316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>low3.csv</td>\n",
       "      <td>344.530933</td>\n",
       "      <td>8.825422</td>\n",
       "      <td>323.176378</td>\n",
       "      <td>8.672358</td>\n",
       "      <td>329.383422</td>\n",
       "      <td>8.667981</td>\n",
       "      <td>356.888795</td>\n",
       "      <td>9.097955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>low4.csv</td>\n",
       "      <td>346.201532</td>\n",
       "      <td>6.674837</td>\n",
       "      <td>325.174021</td>\n",
       "      <td>6.691981</td>\n",
       "      <td>331.199787</td>\n",
       "      <td>6.543173</td>\n",
       "      <td>358.834426</td>\n",
       "      <td>6.628960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>low5.csv</td>\n",
       "      <td>339.212169</td>\n",
       "      <td>3.197527</td>\n",
       "      <td>318.028446</td>\n",
       "      <td>3.150460</td>\n",
       "      <td>324.638831</td>\n",
       "      <td>2.896865</td>\n",
       "      <td>351.652154</td>\n",
       "      <td>3.364131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>low6.csv</td>\n",
       "      <td>340.891293</td>\n",
       "      <td>3.860763</td>\n",
       "      <td>319.359268</td>\n",
       "      <td>3.979781</td>\n",
       "      <td>325.838024</td>\n",
       "      <td>3.831866</td>\n",
       "      <td>352.991073</td>\n",
       "      <td>3.977542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>mid1.csv</td>\n",
       "      <td>342.569604</td>\n",
       "      <td>3.232687</td>\n",
       "      <td>320.881625</td>\n",
       "      <td>2.731930</td>\n",
       "      <td>327.392167</td>\n",
       "      <td>2.864705</td>\n",
       "      <td>354.898542</td>\n",
       "      <td>3.008923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>mid2.csv</td>\n",
       "      <td>339.094550</td>\n",
       "      <td>19.991578</td>\n",
       "      <td>317.414883</td>\n",
       "      <td>20.154398</td>\n",
       "      <td>323.892667</td>\n",
       "      <td>20.235313</td>\n",
       "      <td>348.404383</td>\n",
       "      <td>30.908540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>mid3.csv</td>\n",
       "      <td>348.108681</td>\n",
       "      <td>4.704282</td>\n",
       "      <td>326.701614</td>\n",
       "      <td>4.573676</td>\n",
       "      <td>333.063143</td>\n",
       "      <td>4.413494</td>\n",
       "      <td>360.676710</td>\n",
       "      <td>4.768780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>mid4.csv</td>\n",
       "      <td>351.701679</td>\n",
       "      <td>5.984270</td>\n",
       "      <td>329.798589</td>\n",
       "      <td>6.492790</td>\n",
       "      <td>336.006911</td>\n",
       "      <td>6.413988</td>\n",
       "      <td>364.083036</td>\n",
       "      <td>6.028675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>mid5.csv</td>\n",
       "      <td>361.088510</td>\n",
       "      <td>4.985522</td>\n",
       "      <td>339.524429</td>\n",
       "      <td>5.089271</td>\n",
       "      <td>345.941837</td>\n",
       "      <td>5.032136</td>\n",
       "      <td>373.564490</td>\n",
       "      <td>5.080556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>high1.csv</td>\n",
       "      <td>365.296887</td>\n",
       "      <td>6.410581</td>\n",
       "      <td>343.781145</td>\n",
       "      <td>6.501406</td>\n",
       "      <td>350.144403</td>\n",
       "      <td>6.306159</td>\n",
       "      <td>378.086790</td>\n",
       "      <td>6.520866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>high2.csv</td>\n",
       "      <td>346.894356</td>\n",
       "      <td>5.909033</td>\n",
       "      <td>325.211525</td>\n",
       "      <td>6.042743</td>\n",
       "      <td>331.567271</td>\n",
       "      <td>6.031639</td>\n",
       "      <td>359.142814</td>\n",
       "      <td>6.291320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>high3.csv</td>\n",
       "      <td>340.864409</td>\n",
       "      <td>20.286770</td>\n",
       "      <td>319.690432</td>\n",
       "      <td>20.343321</td>\n",
       "      <td>326.482273</td>\n",
       "      <td>20.467395</td>\n",
       "      <td>356.032489</td>\n",
       "      <td>15.093141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>high4.csv</td>\n",
       "      <td>343.577235</td>\n",
       "      <td>3.208363</td>\n",
       "      <td>322.150147</td>\n",
       "      <td>2.917700</td>\n",
       "      <td>328.555426</td>\n",
       "      <td>2.819510</td>\n",
       "      <td>356.042706</td>\n",
       "      <td>3.071168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>high5.csv</td>\n",
       "      <td>347.958948</td>\n",
       "      <td>5.557481</td>\n",
       "      <td>326.777534</td>\n",
       "      <td>4.503838</td>\n",
       "      <td>332.805379</td>\n",
       "      <td>5.378724</td>\n",
       "      <td>360.789034</td>\n",
       "      <td>5.215512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>high6.csv</td>\n",
       "      <td>345.798073</td>\n",
       "      <td>2.726569</td>\n",
       "      <td>324.504024</td>\n",
       "      <td>2.643870</td>\n",
       "      <td>330.554171</td>\n",
       "      <td>2.754741</td>\n",
       "      <td>358.135951</td>\n",
       "      <td>2.685678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>high7.csv</td>\n",
       "      <td>351.532455</td>\n",
       "      <td>3.322364</td>\n",
       "      <td>330.137015</td>\n",
       "      <td>3.188940</td>\n",
       "      <td>336.203364</td>\n",
       "      <td>3.150839</td>\n",
       "      <td>363.488106</td>\n",
       "      <td>3.209747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1_1_011_V2</td>\n",
       "      <td>end1.csv</td>\n",
       "      <td>350.482763</td>\n",
       "      <td>3.807185</td>\n",
       "      <td>328.946895</td>\n",
       "      <td>3.973402</td>\n",
       "      <td>335.166132</td>\n",
       "      <td>3.970868</td>\n",
       "      <td>362.150079</td>\n",
       "      <td>4.085384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3-3. Stress Index(SI) & Respiratory Rate(RR) Calculate\n",
    "---\n",
    "### Function process_biosignals_rr_si\n",
    "##### - ECG, PPG 데이터를 처리하여 호흡률(RR)과 스트레스 지수(SI) 추출.\n",
    "- Args:\n",
    "    - subject_path (str): 분석할 Subject의 데이터가 담긴 경로.\n",
    "    - signal_type (str): 처리할 신호 종류. 'PPG' 또는 'ECG'.\n",
    "    - save_path (str, optional): 결과를 저장할 최상위 디렉토리 경로.\n",
    "- Returns:\n",
    "    - float | None: 계산된 모든 RR 값들의 전체 평균. 처리된 값이 없으면 None.\n",
    "\n",
    "### Function calculate_si\n",
    "##### - ECG 또는 PPG 신호로부터 Baevsky의 스트레스 지수(SI) 계산.\n",
    "- Args:\n",
    "    - sig (np.ndarray): Raw ECG 또는 PPG 신호 배열.\n",
    "    - fs (float): 신호의 샘플링 레이트 (Hz).\n",
    "    - sig_type (str): 신호 종류 ('ECG' 또는 'PPG').\n",
    "- Returns:\n",
    "    - float: 계산된 스트레스 지수. 계산 불가 시 NaN.\n",
    "### Function calculate_si_feature\n",
    "##### - calculate_si의 예외처리 모듈\n",
    "\n",
    "### Function calculate_rr\n",
    "##### - ECG 또는 PPG 신호의 IBI로부터 호흡률(RR) 계산.\n",
    "- Args:\n",
    "    - sig (np.ndarray): Raw ECG 또는 PPG 신호 배열.\n",
    "    - fs (float): 신호의 샘플링 레이트 (Hz).\n",
    "    - sig_type (str): 신호 종류 ('ECG' 또는 'PPG').\n",
    "- Returns:\n",
    "    - float | None: 분당 호흡수(breaths/min). 계산 불가 시 None.\n",
    "### Function calculate_rr_feature\n",
    "##### - calculate_rr의 예외처리 모듈\n"
   ],
   "id": "cb5770ece7f86d9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:59:48.089153Z",
     "start_time": "2025-09-30T09:59:48.065244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_biosignals_rr_si(subject_path, signal_type, save_path=None):\n",
    "    \"\"\"\n",
    "    ECG, PPG 데이터를 처리하여 호흡률(RR)과 스트레스 지수(SI) 추출.\n",
    "\n",
    "    Args:\n",
    "        subject_path (str): 분석할 Subject의 데이터가 담긴 경로.\n",
    "        signal_type (str): 처리할 신호 종류. 'PPG' 또는 'ECG'.\n",
    "        save_path (str, optional): 결과를 저장할 최상위 디렉토리 경로.\n",
    "\n",
    "    Returns:\n",
    "        float | None: 계산된 모든 RR 값들의 전체 평균. 처리된 값이 없으면 None.\n",
    "    \"\"\"\n",
    "    print(subject_path+' '+signal_type+' Signal - SI, RR 분석 시작')\n",
    "    # 1. 경로 및 파일 목록 설정\n",
    "    if os.path.isfile(subject_path) and subject_path.endswith(\".csv\"):\n",
    "        dir_path = os.path.dirname(subject_path)\n",
    "        file_list = [os.path.basename(subject_path)]\n",
    "        is_file_mode = True\n",
    "    else:\n",
    "        dir_path = subject_path\n",
    "        file_list = [d for d in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, d))]\n",
    "        is_file_mode = False\n",
    "\n",
    "    path_parts = os.path.normpath(subject_path).split(os.sep)\n",
    "    subject_name = path_parts[-3] if is_file_mode else path_parts[-2]\n",
    "\n",
    "    # RR과 SI 결과를 각각 저장할 리스트 초기화\n",
    "    results_list_rr = []\n",
    "    results_list_si = []\n",
    "    all_rr_values = []\n",
    "    all_si_values = []\n",
    "\n",
    "    # 모든 하위 폴더 및 파일을 순회\n",
    "    for subdir_name in file_list:\n",
    "        subdir_path = os.path.join(dir_path, subdir_name)\n",
    "        if not os.path.isdir(subdir_path):\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(subdir_path):\n",
    "            file_path = os.path.join(subdir_path, file_name)\n",
    "            if not file_name.endswith('.csv'):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # CSV 파일 로드 및 신호 추출\n",
    "                raw_df = pd.read_csv(file_path)\n",
    "                raw_signals = extract_signal(raw_df, signal_type)\n",
    "\n",
    "                # wide-format 데이터프레임 구조로 통일\n",
    "                row_data_rr = {\"File\": file_name}\n",
    "                row_data_si = {\"File\": file_name}\n",
    "\n",
    "                # 채널별로 순회하며 RR 및 SI 계산\n",
    "                for channel_name, raw in raw_signals.items():\n",
    "                    # 신호 종류에 맞는 fs 설정\n",
    "                    fs = FS_PPG if signal_type == 'PPG' else FS_ECG\n",
    "\n",
    "                    # RR 및 SI 계산 래퍼 함수 호출\n",
    "                    RR = calculate_rr_feature(raw, fs=fs, sig_type=signal_type)\n",
    "                    SI = calculate_si_feature(raw, fs=fs, sig_type=signal_type)\n",
    "\n",
    "                    # RR 및 SI 결과 저장 (e.g., 'LA-RA_rr', 'LA-RA_si')\n",
    "                    row_data_rr[f\"{channel_name}_rr\"] = RR\n",
    "                    row_data_si[f\"{channel_name}_si\"] = SI\n",
    "                    if RR is not None:\n",
    "                        all_rr_values.append(RR)\n",
    "                    if SI is not None:\n",
    "                        all_si_values.append(SI)\n",
    "\n",
    "                # 한 파일의 결과를 최종 리스트에 저장\n",
    "                results_list_rr.append(row_data_rr)\n",
    "                results_list_si.append(row_data_si)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file_path}: {e}\")\n",
    "    # 모든 결과를 각각의 데이터프레임으로 변환\n",
    "    df_rr = pd.DataFrame(results_list_rr)\n",
    "    df_si = pd.DataFrame(results_list_si)\n",
    "\n",
    "    # 최종 결과를 CSV 파일로 저장\n",
    "    if save_path:\n",
    "        save_dir_rr = os.path.join(save_path, 'RR', signal_type, subject_name)\n",
    "        os.makedirs(save_dir_rr, exist_ok=True) # 폴더를 먼저 생성\n",
    "        save_file_rr = os.path.join(save_dir_rr, f'{subject_name}.csv') # 생성된 폴더 안에 파일 경로 지정\n",
    "        df_rr.to_csv(save_file_rr, index=False)\n",
    "        print(f\"{save_file_rr}에 {signal_type} Signal - RR 분석 결과를 저장했습니다.\")\n",
    "\n",
    "        save_dir_si = os.path.join(save_path, 'SI', signal_type, subject_name)\n",
    "        os.makedirs(save_dir_si, exist_ok=True) # 폴더를 먼저 생성\n",
    "        save_file_si = os.path.join(save_dir_si, f'{subject_name}.csv') # 생성된 폴더 안에 파일 경로 지정\n",
    "        df_si.to_csv(save_file_si, index=False)\n",
    "        print(f\"{save_file_rr}에 {signal_type} Signal - SI 분석 결과를 저장했습니다.\\n\")\n",
    "\n",
    "    return np.mean(all_rr_values) if all_rr_values else None\n",
    "\n",
    "def calculate_si(sig, fs, sig_type):\n",
    "    \"\"\"\n",
    "    ECG 또는 PPG 신호로부터 Baevsky의 스트레스 지수(SI) 계산.\n",
    "\n",
    "    Args:\n",
    "        sig (np.ndarray): Raw ECG 또는 PPG 신호 배열.\n",
    "        fs (float): 신호의 샘플링 레이트 (Hz).\n",
    "        sig_type (str): 신호 종류 ('ECG' 또는 'PPG').\n",
    "\n",
    "    Returns:\n",
    "        float: 계산된 스트레스 지수. 계산 불가 시 NaN.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 신호 종류에 따라 Peak 검출\n",
    "        if sig_type == \"ECG\":\n",
    "            _, m = nk.ecg_process(sig, sampling_rate=fs)\n",
    "            peak_locs = np.asarray(m['ECG_R_Peaks'], dtype=int)\n",
    "        elif sig_type == \"PPG\":\n",
    "            _, m = nk.ppg_process(sig, sampling_rate=fs)\n",
    "            peak_locs = np.asarray(m['PPG_Peaks'], dtype=int)\n",
    "\n",
    "        # IBI (Inter-Beat Interval) 계산\n",
    "        ibi = np.diff(peak_locs) / fs\n",
    "        if len(ibi) == 0:\n",
    "            return float('nan')\n",
    "\n",
    "        # 스트레스 지수(SI) 계산\n",
    "        ibi_counter = Counter(ibi) # IBI 갯수 계산\n",
    "        M0, M0_count = ibi_counter.most_common(1)[0] # IBI의 최빈값(Mode)\n",
    "        AM0 = (M0_count / len(ibi)) * 100 # 최빈값의 진폭(%)\n",
    "        MxDMn = max(ibi) - min(ibi) # 변동 범위(Variation Range)\n",
    "        Stress_Index = sqrt(AM0 / (2 * M0 * MxDMn)) # SI 공식\n",
    "\n",
    "        return Stress_Index\n",
    "    except Exception as e:\n",
    "        print(f\"[heartpy error] {e}\")\n",
    "        return float('nan')\n",
    "\n",
    "def calculate_rr(sig, fs, sig_type):\n",
    "    \"\"\"\n",
    "    ECG 또는 PPG 신호의 IBI로부터 호흡률(RR) 계산.\n",
    "\n",
    "    Args:\n",
    "        sig (np.ndarray): Raw ECG 또는 PPG 신호 배열.\n",
    "        fs (float): 신호의 샘플링 레이트 (Hz).\n",
    "        sig_type (str): 신호 종류 ('ECG' 또는 'PPG').\n",
    "\n",
    "    Returns:\n",
    "        float | None: 분당 호흡수(breaths/min). 계산 불가 시 None.\n",
    "    \"\"\"\n",
    "    # 최소 신호 길이 확인\n",
    "    if len(sig) < fs * 5:\n",
    "        print(\"Warning: Input signal is too short for RR calculation (less than 5 seconds).\")\n",
    "        return None\n",
    "\n",
    "    # 신호 표준화 및 스무딩\n",
    "    if np.std(sig) > 0:\n",
    "        standardized = (sig - np.mean(sig)) / np.std(sig)\n",
    "    else:\n",
    "        standardized = sig # 신호가 평평할 경우 전처리 하지 않음\n",
    "    smoothed = np.convolve(standardized, np.ones(5) / 5, mode='same')\n",
    "\n",
    "    # peak 검출\n",
    "    try:\n",
    "        if sig_type == \"ECG\":\n",
    "            _, m = nk.ecg_process(smoothed, sampling_rate=fs)\n",
    "            peaks = m['ECG_R_Peaks']\n",
    "        elif sig_type == \"PPG\":\n",
    "            _, m = nk.ppg_process(smoothed, sampling_rate=fs)\n",
    "            peaks = m['PPG_Peaks']\n",
    "\n",
    "        # 피크 개수 확인 4개 미만이면 None\n",
    "        if len(peaks) < 4:\n",
    "            print(\"Warning: Not enough peaks found to calculate reliable RR.\")\n",
    "            return None\n",
    "\n",
    "        peak_times = np.asarray(peaks) / fs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Peak detection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # IBI 계산 및 필터링\n",
    "    ibi = np.diff(peak_times)\n",
    "    ibi_times = peak_times[1:]\n",
    "\n",
    "    # 생리학적으로 유효한 IBI 범위\n",
    "    valid_mask = (ibi >= 0.4) & (ibi <= 1.33)\n",
    "    ibi = ibi[valid_mask]\n",
    "    ibi_times = ibi_times[valid_mask]\n",
    "\n",
    "    # 필터링 후 IBI 개수 4개 미만이면 None\n",
    "    if len(ibi) < 4:\n",
    "        print(\"Warning: Not enough valid IBIs after filtering.\")\n",
    "        return None\n",
    "\n",
    "    # Lomb-Scargle Periodogram을 이용한 호흡 주파수 추정\n",
    "    try:\n",
    "        # 분석할 주파수 범위\n",
    "        freqs = np.linspace(0.05, 1.5, 2000)\n",
    "        angular_freqs = 2 * np.pi * freqs\n",
    "        ibi_mean_removed = ibi - np.mean(ibi)\n",
    "        psd = lombscargle(ibi_times, ibi_mean_removed, angular_freqs)\n",
    "\n",
    "        # 호흡 대역(HF: 0.15-0.4Hz)에서 가장 강한 주파수 탐색\n",
    "        hf_mask = (freqs >= 0.15) & (freqs <= 0.4)\n",
    "        hf_freqs = freqs[hf_mask]\n",
    "        hf_psd = psd[hf_mask]\n",
    "        if len(hf_psd) == 0:\n",
    "            return None\n",
    "\n",
    "        peak_idx = np.argmax(hf_psd)\n",
    "        peak_freq = hf_freqs[peak_idx]\n",
    "\n",
    "        # 분당 호흡수로 변환\n",
    "        rr_bpm = peak_freq * 60\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lomb-Scargle calculation failed: {e}\")\n",
    "        rr_bpm = None\n",
    "\n",
    "    return rr_bpm\n",
    "\n",
    "def calculate_rr_feature(signal, fs, sig_type):\n",
    "    \"\"\"\n",
    "    calculate_rr 함수의 예외처리를 당담\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rr = calculate_rr(signal, fs, sig_type)\n",
    "        return rr\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR - get_{sig_type}_RR] {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_si_feature(signal, fs, sig_type):\n",
    "    \"\"\"\n",
    "    calculate_si 함수의 예외처리를 당담\n",
    "    \"\"\"\n",
    "    try:\n",
    "        si = calculate_si(signal, fs, sig_type)\n",
    "        return si\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR - get_{sig_type}_SI] {e}\")\n",
    "        return None"
   ],
   "id": "82fcf42a549d0101",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:01:22.900083Z",
     "start_time": "2025-09-30T09:59:48.110432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_dir = r\"../features/\"\n",
    "split_data_dir = r\"../data/split_data/\"\n",
    "_ = process_biosignals_rr_si(split_data_dir+subject+r'\\\\ECG', 'ECG', save_path=save_dir)\n",
    "_ = process_biosignals_rr_si(split_data_dir+subject+r'\\\\PPG', 'PPG', save_path=save_dir)\n",
    "\n",
    "RR_ECG_result = pd.read_csv(r'..\\features\\RR\\ECG\\\\'+subject+'\\\\'+subject+r'.csv')\n",
    "RR_PPG_result = pd.read_csv(r'..\\features\\RR\\PPG\\\\'+subject+'\\\\'+subject+r'.csv')\n",
    "\n",
    "SI_ECG_result = pd.read_csv(r'..\\features\\SI\\ECG\\\\'+subject+'\\\\'+subject+r'.csv')\n",
    "SI_PPG_result = pd.read_csv(r'..\\features\\SI\\PPG\\\\'+subject+'\\\\'+subject+r'.csv')"
   ],
   "id": "b8e1940e38b12b44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/split_data/1_1_011_V2\\\\ECG ECG Signal - SI, RR 분석 시작\n",
      "../features/RR\\ECG\\1_1_011_V2\\1_1_011_V2.csv에 ECG Signal - RR 분석 결과를 저장했습니다.\n",
      "../features/RR\\ECG\\1_1_011_V2\\1_1_011_V2.csv에 ECG Signal - SI 분석 결과를 저장했습니다.\n",
      "\n",
      "../data/split_data/1_1_011_V2\\\\PPG PPG Signal - SI, RR 분석 시작\n",
      "../features/RR\\PPG\\1_1_011_V2\\1_1_011_V2.csv에 PPG Signal - RR 분석 결과를 저장했습니다.\n",
      "../features/RR\\PPG\\1_1_011_V2\\1_1_011_V2.csv에 PPG Signal - SI 분석 결과를 저장했습니다.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:01:22.987699Z",
     "start_time": "2025-09-30T10:01:22.968682Z"
    }
   },
   "cell_type": "code",
   "source": "RR_ECG_result",
   "id": "2328b861515e58f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          File   LA_RA_rr   LL_LA_rr   LL_RA_rr   Vx_RL_rr\n",
       "0     end1.csv  21.801401  21.801401  21.757879  21.801401\n",
       "1    high1.csv  23.977489  23.977489  23.977489  23.977489\n",
       "2    high2.csv  22.410705  22.367184  22.367184  22.410705\n",
       "3    high3.csv  21.540270  21.583792  21.583792  21.018009\n",
       "4    high4.csv  23.672836  23.672836  23.716358  23.672836\n",
       "5    high5.csv  23.977489  23.977489  23.977489  23.977489\n",
       "6    high6.csv  12.226613  12.226613  12.226613  12.226613\n",
       "7    high7.csv  23.977489  23.977489  23.977489  23.977489\n",
       "8     low1.csv  22.367184  22.367184  22.323662  22.367184\n",
       "9     low2.csv  17.753877  17.710355  17.710355  17.710355\n",
       "10    low3.csv  20.756878  20.756878  20.800400  20.756878\n",
       "11    low4.csv  21.496748  21.453227  21.453227  21.453227\n",
       "12    low5.csv  21.757879  21.714357  21.714357  21.714357\n",
       "13    low6.csv  18.972486  18.972486  18.972486  18.972486\n",
       "14    mid1.csv  21.148574  21.148574  21.148574  21.148574\n",
       "15    mid2.csv  18.319660  18.319660  18.319660  18.276138\n",
       "16    mid3.csv  19.407704  19.277139  19.407704  19.277139\n",
       "17    mid4.csv  20.234617  20.234617  20.321661  20.278139\n",
       "18    mid5.csv  22.715358  22.758879  22.715358  22.758879\n",
       "19  start1.csv  10.311656  10.311656  10.311656  10.311656"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>LA_RA_rr</th>\n",
       "      <th>LL_LA_rr</th>\n",
       "      <th>LL_RA_rr</th>\n",
       "      <th>Vx_RL_rr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end1.csv</td>\n",
       "      <td>21.801401</td>\n",
       "      <td>21.801401</td>\n",
       "      <td>21.757879</td>\n",
       "      <td>21.801401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high1.csv</td>\n",
       "      <td>23.977489</td>\n",
       "      <td>23.977489</td>\n",
       "      <td>23.977489</td>\n",
       "      <td>23.977489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high2.csv</td>\n",
       "      <td>22.410705</td>\n",
       "      <td>22.367184</td>\n",
       "      <td>22.367184</td>\n",
       "      <td>22.410705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high3.csv</td>\n",
       "      <td>21.540270</td>\n",
       "      <td>21.583792</td>\n",
       "      <td>21.583792</td>\n",
       "      <td>21.018009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high4.csv</td>\n",
       "      <td>23.672836</td>\n",
       "      <td>23.672836</td>\n",
       "      <td>23.716358</td>\n",
       "      <td>23.672836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>high5.csv</td>\n",
       "      <td>23.977489</td>\n",
       "      <td>23.977489</td>\n",
       "      <td>23.977489</td>\n",
       "      <td>23.977489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>high6.csv</td>\n",
       "      <td>12.226613</td>\n",
       "      <td>12.226613</td>\n",
       "      <td>12.226613</td>\n",
       "      <td>12.226613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>high7.csv</td>\n",
       "      <td>23.977489</td>\n",
       "      <td>23.977489</td>\n",
       "      <td>23.977489</td>\n",
       "      <td>23.977489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>low1.csv</td>\n",
       "      <td>22.367184</td>\n",
       "      <td>22.367184</td>\n",
       "      <td>22.323662</td>\n",
       "      <td>22.367184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>low2.csv</td>\n",
       "      <td>17.753877</td>\n",
       "      <td>17.710355</td>\n",
       "      <td>17.710355</td>\n",
       "      <td>17.710355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>low3.csv</td>\n",
       "      <td>20.756878</td>\n",
       "      <td>20.756878</td>\n",
       "      <td>20.800400</td>\n",
       "      <td>20.756878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>low4.csv</td>\n",
       "      <td>21.496748</td>\n",
       "      <td>21.453227</td>\n",
       "      <td>21.453227</td>\n",
       "      <td>21.453227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>low5.csv</td>\n",
       "      <td>21.757879</td>\n",
       "      <td>21.714357</td>\n",
       "      <td>21.714357</td>\n",
       "      <td>21.714357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>low6.csv</td>\n",
       "      <td>18.972486</td>\n",
       "      <td>18.972486</td>\n",
       "      <td>18.972486</td>\n",
       "      <td>18.972486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mid1.csv</td>\n",
       "      <td>21.148574</td>\n",
       "      <td>21.148574</td>\n",
       "      <td>21.148574</td>\n",
       "      <td>21.148574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mid2.csv</td>\n",
       "      <td>18.319660</td>\n",
       "      <td>18.319660</td>\n",
       "      <td>18.319660</td>\n",
       "      <td>18.276138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mid3.csv</td>\n",
       "      <td>19.407704</td>\n",
       "      <td>19.277139</td>\n",
       "      <td>19.407704</td>\n",
       "      <td>19.277139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mid4.csv</td>\n",
       "      <td>20.234617</td>\n",
       "      <td>20.234617</td>\n",
       "      <td>20.321661</td>\n",
       "      <td>20.278139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mid5.csv</td>\n",
       "      <td>22.715358</td>\n",
       "      <td>22.758879</td>\n",
       "      <td>22.715358</td>\n",
       "      <td>22.758879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>start1.csv</td>\n",
       "      <td>10.311656</td>\n",
       "      <td>10.311656</td>\n",
       "      <td>10.311656</td>\n",
       "      <td>10.311656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:01:23.096373Z",
     "start_time": "2025-09-30T10:01:23.087259Z"
    }
   },
   "cell_type": "code",
   "source": "RR_PPG_result",
   "id": "40dbff602fd5a67d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          File     ppg_rr\n",
       "0     end1.csv  21.670835\n",
       "1    high1.csv  23.977489\n",
       "2    high2.csv  22.410705\n",
       "3    high3.csv  21.105053\n",
       "4    high4.csv  23.759880\n",
       "5    high5.csv  23.977489\n",
       "6    high6.csv  21.931966\n",
       "7    high7.csv  23.977489\n",
       "8     low1.csv  22.367184\n",
       "9     low2.csv  17.840920\n",
       "10    low3.csv  20.278139\n",
       "11    low4.csv  20.974487\n",
       "12    low5.csv  21.714357\n",
       "13    low6.csv  18.754877\n",
       "14    mid1.csv  21.192096\n",
       "15    mid2.csv  18.102051\n",
       "16    mid3.csv  19.451226\n",
       "17    mid4.csv  20.147574\n",
       "18    mid5.csv  22.628314\n",
       "19  start1.csv  10.224612"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>ppg_rr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end1.csv</td>\n",
       "      <td>21.670835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high1.csv</td>\n",
       "      <td>23.977489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high2.csv</td>\n",
       "      <td>22.410705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high3.csv</td>\n",
       "      <td>21.105053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high4.csv</td>\n",
       "      <td>23.759880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>high5.csv</td>\n",
       "      <td>23.977489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>high6.csv</td>\n",
       "      <td>21.931966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>high7.csv</td>\n",
       "      <td>23.977489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>low1.csv</td>\n",
       "      <td>22.367184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>low2.csv</td>\n",
       "      <td>17.840920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>low3.csv</td>\n",
       "      <td>20.278139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>low4.csv</td>\n",
       "      <td>20.974487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>low5.csv</td>\n",
       "      <td>21.714357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>low6.csv</td>\n",
       "      <td>18.754877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mid1.csv</td>\n",
       "      <td>21.192096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mid2.csv</td>\n",
       "      <td>18.102051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mid3.csv</td>\n",
       "      <td>19.451226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mid4.csv</td>\n",
       "      <td>20.147574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mid5.csv</td>\n",
       "      <td>22.628314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>start1.csv</td>\n",
       "      <td>10.224612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:01:23.315070Z",
     "start_time": "2025-09-30T10:01:23.304858Z"
    }
   },
   "cell_type": "code",
   "source": "SI_ECG_result",
   "id": "f405cacf52fca2c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          File  LA_RA_si  LL_LA_si  LL_RA_si  Vx_RL_si\n",
       "0     end1.csv  6.049120  4.951542  5.599995  5.453788\n",
       "1    high1.csv  7.041724  6.278097  6.371342  6.371342\n",
       "2    high2.csv  5.953154  5.960647  6.433375  6.485050\n",
       "3    high3.csv  4.328427  4.263869  4.303813  4.348189\n",
       "4    high4.csv  5.472075  5.402364  4.837811  4.900237\n",
       "5    high5.csv  4.962387  4.766804  4.653969  4.687572\n",
       "6    high6.csv  5.624757  5.737546  4.895061  4.871183\n",
       "7    high7.csv  4.958591  5.008302  3.899336  5.578415\n",
       "8     low1.csv  8.736642  7.210610  6.244571  8.061707\n",
       "9     low2.csv  6.629277  5.992449  5.992449  5.886282\n",
       "10    low3.csv  3.949179  3.933600  3.929086  3.913708\n",
       "11    low4.csv  4.596123  4.735244  4.735244  4.774823\n",
       "12    low5.csv  5.443114  5.439181  5.439181  6.124472\n",
       "13    low6.csv  5.008260  4.900176  5.658236  5.035703\n",
       "14    mid1.csv  8.028274  6.468112  7.328778  7.270381\n",
       "15    mid2.csv  4.168209  4.142080  4.176455  3.702512\n",
       "16    mid3.csv  4.024904  4.652984  4.681618  4.800147\n",
       "17    mid4.csv  7.536508  6.740858  7.611500  5.781349\n",
       "18    mid5.csv  6.679726  4.666264  4.763617  6.736096\n",
       "19  start1.csv  3.341238  3.853351  3.836706  3.853351"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>LA_RA_si</th>\n",
       "      <th>LL_LA_si</th>\n",
       "      <th>LL_RA_si</th>\n",
       "      <th>Vx_RL_si</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end1.csv</td>\n",
       "      <td>6.049120</td>\n",
       "      <td>4.951542</td>\n",
       "      <td>5.599995</td>\n",
       "      <td>5.453788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high1.csv</td>\n",
       "      <td>7.041724</td>\n",
       "      <td>6.278097</td>\n",
       "      <td>6.371342</td>\n",
       "      <td>6.371342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high2.csv</td>\n",
       "      <td>5.953154</td>\n",
       "      <td>5.960647</td>\n",
       "      <td>6.433375</td>\n",
       "      <td>6.485050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high3.csv</td>\n",
       "      <td>4.328427</td>\n",
       "      <td>4.263869</td>\n",
       "      <td>4.303813</td>\n",
       "      <td>4.348189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high4.csv</td>\n",
       "      <td>5.472075</td>\n",
       "      <td>5.402364</td>\n",
       "      <td>4.837811</td>\n",
       "      <td>4.900237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>high5.csv</td>\n",
       "      <td>4.962387</td>\n",
       "      <td>4.766804</td>\n",
       "      <td>4.653969</td>\n",
       "      <td>4.687572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>high6.csv</td>\n",
       "      <td>5.624757</td>\n",
       "      <td>5.737546</td>\n",
       "      <td>4.895061</td>\n",
       "      <td>4.871183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>high7.csv</td>\n",
       "      <td>4.958591</td>\n",
       "      <td>5.008302</td>\n",
       "      <td>3.899336</td>\n",
       "      <td>5.578415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>low1.csv</td>\n",
       "      <td>8.736642</td>\n",
       "      <td>7.210610</td>\n",
       "      <td>6.244571</td>\n",
       "      <td>8.061707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>low2.csv</td>\n",
       "      <td>6.629277</td>\n",
       "      <td>5.992449</td>\n",
       "      <td>5.992449</td>\n",
       "      <td>5.886282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>low3.csv</td>\n",
       "      <td>3.949179</td>\n",
       "      <td>3.933600</td>\n",
       "      <td>3.929086</td>\n",
       "      <td>3.913708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>low4.csv</td>\n",
       "      <td>4.596123</td>\n",
       "      <td>4.735244</td>\n",
       "      <td>4.735244</td>\n",
       "      <td>4.774823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>low5.csv</td>\n",
       "      <td>5.443114</td>\n",
       "      <td>5.439181</td>\n",
       "      <td>5.439181</td>\n",
       "      <td>6.124472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>low6.csv</td>\n",
       "      <td>5.008260</td>\n",
       "      <td>4.900176</td>\n",
       "      <td>5.658236</td>\n",
       "      <td>5.035703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mid1.csv</td>\n",
       "      <td>8.028274</td>\n",
       "      <td>6.468112</td>\n",
       "      <td>7.328778</td>\n",
       "      <td>7.270381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mid2.csv</td>\n",
       "      <td>4.168209</td>\n",
       "      <td>4.142080</td>\n",
       "      <td>4.176455</td>\n",
       "      <td>3.702512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mid3.csv</td>\n",
       "      <td>4.024904</td>\n",
       "      <td>4.652984</td>\n",
       "      <td>4.681618</td>\n",
       "      <td>4.800147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mid4.csv</td>\n",
       "      <td>7.536508</td>\n",
       "      <td>6.740858</td>\n",
       "      <td>7.611500</td>\n",
       "      <td>5.781349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mid5.csv</td>\n",
       "      <td>6.679726</td>\n",
       "      <td>4.666264</td>\n",
       "      <td>4.763617</td>\n",
       "      <td>6.736096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>start1.csv</td>\n",
       "      <td>3.341238</td>\n",
       "      <td>3.853351</td>\n",
       "      <td>3.836706</td>\n",
       "      <td>3.853351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:01:23.416951Z",
     "start_time": "2025-09-30T10:01:23.409949Z"
    }
   },
   "cell_type": "code",
   "source": "SI_PPG_result",
   "id": "78b0da014b9ccf57",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          File     ppg_si\n",
       "0     end1.csv  12.395060\n",
       "1    high1.csv  12.075019\n",
       "2    high2.csv  10.539353\n",
       "3    high3.csv  11.639370\n",
       "4    high4.csv  10.361363\n",
       "5    high5.csv  10.083599\n",
       "6    high6.csv  15.111202\n",
       "7    high7.csv   8.533333\n",
       "8     low1.csv  12.935454\n",
       "9     low2.csv  11.062677\n",
       "10    low3.csv  11.777119\n",
       "11    low4.csv   9.436285\n",
       "12    low5.csv  10.248202\n",
       "13    low6.csv   7.807927\n",
       "14    mid1.csv  13.326828\n",
       "15    mid2.csv  11.919877\n",
       "16    mid3.csv  10.768640\n",
       "17    mid4.csv  13.205119\n",
       "18    mid5.csv  12.903645\n",
       "19  start1.csv   6.782796"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>ppg_si</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>end1.csv</td>\n",
       "      <td>12.395060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high1.csv</td>\n",
       "      <td>12.075019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high2.csv</td>\n",
       "      <td>10.539353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high3.csv</td>\n",
       "      <td>11.639370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high4.csv</td>\n",
       "      <td>10.361363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>high5.csv</td>\n",
       "      <td>10.083599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>high6.csv</td>\n",
       "      <td>15.111202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>high7.csv</td>\n",
       "      <td>8.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>low1.csv</td>\n",
       "      <td>12.935454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>low2.csv</td>\n",
       "      <td>11.062677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>low3.csv</td>\n",
       "      <td>11.777119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>low4.csv</td>\n",
       "      <td>9.436285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>low5.csv</td>\n",
       "      <td>10.248202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>low6.csv</td>\n",
       "      <td>7.807927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mid1.csv</td>\n",
       "      <td>13.326828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>mid2.csv</td>\n",
       "      <td>11.919877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mid3.csv</td>\n",
       "      <td>10.768640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mid4.csv</td>\n",
       "      <td>13.205119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mid5.csv</td>\n",
       "      <td>12.903645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>start1.csv</td>\n",
       "      <td>6.782796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3-4. GSR Feature\n",
    "---\n",
    "### Function process_biosignals_gsr\n",
    "##### - 한 Subject의 모든 GSR(EDA) 파일을 분석하여 EDA 지표를 계산하고 결과 반환.\n",
    "- Args:\n",
    "    - subject_path (str): 분석할 Subject의 데이터가 담긴 경로.\n",
    "    - save_path (str, optional): 분석 결과를 저장할 디렉토리 경로.\n",
    "- Returns:\n",
    "    - pd.DataFrame | None:\n",
    "        - Subject의 모든 파일에 대한 EDA 분석 결과가 포함된 데이터프레임.\n",
    "        - 처리할 데이터가 없는 경우 None을 반환.\n",
    "\n",
    "### Function calculate_scr_scl\n",
    "##### - GSR 신호로부터 SCR, SCL 및 관련 지표 계산.\n",
    "- Args:\n",
    "    - sig (np.ndarray): Raw GSR 신호 배열.\n",
    "    - fs (float): 신호의 샘플링 레이트 (Hz).\n",
    "    - sig_type (str): 신호 종류 (현재 'GSR'만 지원).\n",
    "- Returns:\n",
    "    - pd.DataFrame | None: EDA 분석 결과가 담긴 DataFrame. 오류 시 None."
   ],
   "id": "8912787135de6be6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:04:26.723170Z",
     "start_time": "2025-09-30T10:04:26.713948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NeuroKit2에서 발생하는 특정 경고만 무시하도록 설정\n",
    "warnings.filterwarnings(\"ignore\", category=NeuroKitWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "def process_biosignals_gsr(subject_path, save_path=None):\n",
    "    \"\"\"\n",
    "    한 Subject의 모든 GSR(EDA) 파일을 분석하여 EDA 지표를 계산하고 결과 반환.\n",
    "\n",
    "    Args:\n",
    "        subject_path (str): 분석할 Subject의 데이터가 담긴 경로.\n",
    "        save_path (str, optional): 분석 결과를 저장할 디렉토리 경로.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None:\n",
    "            Subject의 모든 파일에 대한 EDA 분석 결과가 포함된 데이터프레임.\n",
    "            처리할 데이터가 없는 경우 None을 반환.\n",
    "    \"\"\"\n",
    "    print(subject_path+' GSR Signal - 분석 시작')\n",
    "    # 입력 경로가 파일인지 폴더인지 확인하고, 처리할 파일 목록 설정\n",
    "    if os.path.isfile(subject_path) and subject_path.endswith(\".csv\"):\n",
    "        file_list = [os.path.basename(subject_path)]\n",
    "        is_file_mode = True\n",
    "    else:\n",
    "        dir_path = subject_path\n",
    "        file_list = [d for d in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, d))]\n",
    "        is_file_mode = False\n",
    "\n",
    "    # Subject 이름 추출\n",
    "    path_parts = os.path.normpath(subject_path).split(os.sep)\n",
    "    subject_name = path_parts[-3] if is_file_mode else path_parts[-2]\n",
    "\n",
    "\n",
    "    # 최종 결과를 저장할 빈 리스트 초기화\n",
    "    results_list = []\n",
    "\n",
    "    # 모든 파일을 순회하는 반복문\n",
    "    for subdir_name in file_list:\n",
    "        subdir_path = os.path.join(subject_path, subdir_name)\n",
    "        for file_name in os.listdir(subdir_path):\n",
    "            # .csv로 끝나지 않으면 데이터로 인식하지 않고 넘김\n",
    "            if not file_name.endswith('.csv'):\n",
    "                continue\n",
    "\n",
    "            # .csv로 끝났다면 경로 저장\n",
    "            file_path = os.path.join(subdir_path, file_name)\n",
    "\n",
    "            # CSV 파일 로드 및 GSR 신호 추출\n",
    "            raw_df = pd.read_csv(file_path)\n",
    "            raw_signal_dict = extract_signal(raw_df, data_type = 'GSR')\n",
    "\n",
    "            # 각 채널별로 EDA 지표 계산\n",
    "            for channel_name, signal_array in raw_signal_dict.items():\n",
    "                # SCR, SCL 등 EDA 특징 계산\n",
    "                eda_metrics_df = calculate_scr_scl(signal_array, fs=51.2, sig_type='GSR')\n",
    "\n",
    "                # 결과 정리 및 리스트에 추가\n",
    "                if eda_metrics_df is not None:\n",
    "                    # DataFrame가 None이 아니라면 결과를 딕셔너리로 변환\n",
    "                    eda_metrics_dict = eda_metrics_df.to_dict(orient='records')[0]\n",
    "                    result_data = {'File': file_name, **eda_metrics_dict}\n",
    "                    results_list.append(result_data)\n",
    "\n",
    "    if not results_list:\n",
    "        print(\"No data was processed.\")\n",
    "        return None\n",
    "    # 모든 결과를 하나의 데이터프레임으로 변환\n",
    "    final_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # 최종 결과를 CSV 파일로 저장 (save_path가 지정된 경우)\n",
    "    if save_path:\n",
    "        save_dir = os.path.join(save_path, 'EDA', subject_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_file = os.path.join(save_dir, f'{subject_name}.csv')\n",
    "        final_df.to_csv(save_file, index=False)\n",
    "        print(f\"{save_file}에 GSR Signal - 분석 결과를 저장했습니다.\\n\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "def calculate_scr_scl(sig, fs, sig_type):\n",
    "    \"\"\"\n",
    "    GSR 신호로부터 SCR, SCL 및 관련 지표 계산.\n",
    "\n",
    "    Args:\n",
    "        sig (np.ndarray): Raw GSR 신호 배열.\n",
    "        fs (float): 신호의 샘플링 레이트 (Hz).\n",
    "        sig_type (str): 신호 종류 (현재 'GSR'만 지원).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | None: EDA 분석 결과가 담긴 DataFrame. 오류 시 None.\n",
    "    \"\"\"\n",
    "    # Z-score 정규화\n",
    "    smoothed = (sig - np.mean(sig)) / np.std(sig)\n",
    "    try:\n",
    "        if sig_type == \"GSR\":\n",
    "            # GSR 전처리\n",
    "            sig, m = nk.eda_process(smoothed, sampling_rate=fs)\n",
    "            analyze_df = nk.eda_analyze(sig, sampling_rate=fs)\n",
    "\n",
    "            return analyze_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Wrong signals: {e}\")\n",
    "        return None"
   ],
   "id": "5b1ecacc11082ae8",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:04:27.451441Z",
     "start_time": "2025-09-30T10:04:26.987675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_dir = r\"../features/\"\n",
    "split_data_dir = r\"../data/split_data/\"\n",
    "\n",
    "GSR_result = process_biosignals_gsr(split_data_dir+ subject+r'\\\\PPG', save_path=save_dir)\n",
    "GSR_result"
   ],
   "id": "fb7a38175663a891",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/split_data/1_1_011_V2\\\\PPG GSR Signal - 분석 시작\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "Wrong signals: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
      "No data was processed.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. 분석 데이터 준비\n",
    "### 4-1. Feature Integration\n",
    "---\n",
    "### Function load_feature_csv\n",
    "##### - Feature CSV 파일을 로드하고 기본적인 전처리를 수행.\n",
    "- Args:\n",
    "    - path (str): 로드할 CSV 파일의 전체 경로.\n",
    "    - drop_cols (list[str], optional): 제거할 컬럼 이름의 리스트.\n",
    "    - pivot (bool, optional): 채널(channel) 기준으로 피벗을 수행할지 여부.\n",
    "    - index_col (str, optional): DataFrame의 인덱스로 설정할 컬럼명.\n",
    "- Returns:\n",
    "    - pd.DataFrame: 전처리된 Feature 데이터프레임.\n",
    "\n",
    "### Function load_feature_csv\n",
    "##### - Feature CSV 파일을 로드하고 기본적인 전처리를 수행.\n",
    "- Args:\n",
    "    - path (str): 로드할 CSV 파일의 전체 경로.\n",
    "    - drop_cols (list[str], optional): 제거할 컬럼 이름의 리스트.\n",
    "    - pivot (bool, optional): 채널(channel) 기준으로 피벗을 수행할지 여부.\n",
    "    - index_col (str, optional): DataFrame의 인덱스로 설정할 컬럼명.\n",
    "\n",
    "### Function save_joined_features\n",
    "##### - 특정 Subject의 모든 분산된 Feature들을 하나로 통합하여 단일 CSV 파일로 저장.\n",
    "- Args:\n",
    "    - subject (str, optional): 처리할 Subject의 ID.\n",
    "    - save_dir (str, optional): 통합된 CSV 파일을 저장할 디렉토리 경로.\n",
    "\n",
    "### Function load_subject_data\n",
    "##### - 한 subject의 joined 데이터와 label 데이터를 불러와 병합 준비.\n",
    "- Args:\n",
    "    - fname (str): CSV 파일 이름 (예: \"sub1.csv\").\n",
    "    - joined_dir (str): joined feature 파일들이 있는 디렉토리 경로.\n",
    "    - label_dir (str): label 파일들이 있는 디렉토리 경로.\n",
    "- Returns:\n",
    "    - tuple[pd.DataFrame, pd.DataFrame] | None:\n",
    "        - joined_df (DataFrame): feature 데이터.\n",
    "        - label_df (DataFrame): label 데이터.\n",
    "        - 파일이 존재하지 않으면 None 반환."
   ],
   "id": "2dfae17f9256b38f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:01:24.300813Z",
     "start_time": "2025-09-30T10:01:24.288639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_feature_csv(path, subject, drop_cols=None, pivot=False, index_col=\"File\"):\n",
    "    \"\"\"\n",
    "    Feature CSV 파일을 로드하고 기본적인 전처리를 수행.\n",
    "\n",
    "    Args:\n",
    "        path (str): 로드할 CSV 파일의 전체 경로.\n",
    "        drop_cols (list[str], optional): 제거할 컬럼 이름의 리스트.\n",
    "        pivot (bool, optional): 채널(channel) 기준으로 피벗을 수행할지 여부.\n",
    "        index_col (str, optional): DataFrame의 인덱스로 설정할 컬럼명.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 전처리된 Feature 데이터프레임.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # end/start 파일 제거\n",
    "    if \"File\" in df.columns:\n",
    "        df = df[~df['File'].str.contains('end|start')].copy()\n",
    "\n",
    "    # 불필요한 column 제거\n",
    "    if drop_cols:\n",
    "        df = df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "    # pivot 수행\n",
    "    if pivot and \"channel\" in df.columns:\n",
    "        df = df.pivot(\n",
    "            index=\"File\",\n",
    "            columns=\"channel\",\n",
    "            values=[c for c in df.columns if c not in [\"File\", \"channel\"]]\n",
    "        )\n",
    "        # 다중 컬럼 평탄화\n",
    "        df.columns = [f\"{channel}_{feature}\" for feature, channel in df.columns]\n",
    "        df = df.reset_index()\n",
    "\n",
    "    # index 지정\n",
    "    if index_col in df.columns:\n",
    "        df = df.set_index(index_col)\n",
    "\n",
    "    return df\n",
    "\n",
    "def save_joined_features(subject = \"1_1_001_V2\", save_dir = \"../features/joined\"):\n",
    "    \"\"\"\n",
    "    특정 Subject의 모든 분산된 Feature들을 하나로 통합하여 단일 CSV 파일로 저장.\n",
    "\n",
    "    Args:\n",
    "        subject (str, optional): 처리할 Subject의 ID.\n",
    "        save_dir (str, optional): 통합된 CSV 파일을 저장할 디렉토리 경로.\n",
    "\n",
    "    Returns:\n",
    "        None:\n",
    "            이 함수는 값을 반환하지 않고, 결과를 파일로 저장.\n",
    "    \"\"\"\n",
    "    print(f\"{subject} 추출한 특징 파일 병합 시작\")\n",
    "    # ---- 메인 코드 ----\n",
    "    EDA_path    = r\"../features/EDA\"\n",
    "    HR_HRV_path = r\"../features/HR_HRV\"\n",
    "    PTT_path    = r\"../features/PTT\"\n",
    "    RR_path     = r\"../features/RR\"\n",
    "    SI_path     = r\"../features/SI\"\n",
    "    label_path  = r\"../features/label\"\n",
    "\n",
    "    # 1. EDA\n",
    "    EDA_feature = load_feature_csv(f\"{EDA_path}/{subject}/{subject}.csv\", subject)\n",
    "    # print(EDA_feature.shape)\n",
    "\n",
    "    # 2. HR_HRV ECG\n",
    "    HR_HRV_ECG_feature = load_feature_csv(f\"{HR_HRV_path}/ECG/{subject}/{subject}.csv\", subject, pivot=True)\n",
    "    # print(HR_HRV_ECG_feature.shape)\n",
    "\n",
    "    # 3. HR_HRV PPG\n",
    "    HR_HRV_PPG_feature = load_feature_csv(f\"{HR_HRV_path}/PPG/{subject}/{subject}.csv\", subject, pivot=True)\n",
    "    # print(HR_HRV_PPG_feature.shape)\n",
    "\n",
    "    # 4. PTT\n",
    "    PTT_feature = load_feature_csv(f\"{PTT_path}/{subject}.csv\", subject, drop_cols=[ \"Subject\"])\n",
    "    # print(PTT_feature.shape)\n",
    "\n",
    "    # 5. RR ECG\n",
    "    RR_ECG_feature = load_feature_csv(f\"{RR_path}/ECG/{subject}/{subject}.csv\", subject)\n",
    "    # print(RR_ECG_feature.shape)\n",
    "\n",
    "    # 6. RR PPG\n",
    "    RR_PPG_feature = load_feature_csv(f\"{RR_path}/PPG/{subject}/{subject}.csv\", subject)\n",
    "    # print(RR_PPG_feature.shape)\n",
    "\n",
    "    # 7. SI ECG\n",
    "    SI_ECG_feature = load_feature_csv(f\"{SI_path}/ECG/{subject}/{subject}.csv\", subject)\n",
    "    # print(SI_ECG_feature.shape)\n",
    "\n",
    "    # 8. SI PPG\n",
    "    SI_PPG_feature = load_feature_csv(f\"{SI_path}/PPG/{subject}/{subject}.csv\", subject)\n",
    "    # print(SI_PPG_feature.shape)\n",
    "\n",
    "    # ---- 모든 feature 병합 ----\n",
    "    joined_df = (\n",
    "        EDA_feature\n",
    "        .join(HR_HRV_ECG_feature, how=\"outer\")\n",
    "        .join(HR_HRV_PPG_feature, how=\"outer\")\n",
    "        .join(PTT_feature, how=\"outer\")\n",
    "        .join(RR_ECG_feature, how=\"outer\")\n",
    "        .join(RR_PPG_feature, how=\"outer\")\n",
    "        .join(SI_ECG_feature, how=\"outer\")\n",
    "        .join(SI_PPG_feature, how=\"outer\")\n",
    "    )\n",
    "\n",
    "    # 정렬 우선순위 정의\n",
    "    order = {\"low\": 0, \"mid\": 1, \"high\": 2}\n",
    "\n",
    "    def sort_key(fname):\n",
    "\n",
    "        name = str(fname).lower()\n",
    "        # group (low/mid/high)\n",
    "        group = None\n",
    "        for k in order:\n",
    "            if k in name:\n",
    "                group = k\n",
    "                break\n",
    "        # 파일명에서 숫자 추출\n",
    "        m = re.search(r'(\\d+)', name)\n",
    "        num = int(m.group(1)) if m else 0\n",
    "        return (order.get(group, 99), num)\n",
    "\n",
    "    # 행 인덱스 정렬\n",
    "    joined_df = joined_df.reindex(sorted(joined_df.index, key=sort_key))\n",
    "\n",
    "    # print(joined_df)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"{subject}.csv\")\n",
    "\n",
    "    # CSV 저장\n",
    "    joined_df.to_csv(save_path, index=True)\n",
    "\n",
    "    print(f\"{save_path}에 병합된 특징 CSV 파일을 저장하였습니다.\\n\")\n",
    "\n",
    "def load_subject_data(fname, joined_dir = r\"../features/joined\", label_dir = r\"../features/label\"):\n",
    "    \"\"\"\n",
    "    한 subject의 joined 데이터와 label 데이터를 불러와 병합 준비.\n",
    "\n",
    "    Args:\n",
    "        fname (str): CSV 파일 이름 (예: \"sub1.csv\").\n",
    "        joined_dir (str): joined feature 파일들이 있는 디렉토리 경로.\n",
    "        label_dir (str): label 파일들이 있는 디렉토리 경로.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame] | None:\n",
    "            joined_df (DataFrame): feature 데이터.\n",
    "            label_df (DataFrame): label 데이터 (File, Q1, Q2만 포함).\n",
    "            파일이 존재하지 않으면 None 반환.\n",
    "    \"\"\"\n",
    "    print(fname+' 특징과 Label을 로드합니다.')\n",
    "    # joined feature 파일 경로\n",
    "    jp = os.path.join(joined_dir, fname)\n",
    "    # label 파일 경로\n",
    "    lp = os.path.join(label_dir, fname)\n",
    "\n",
    "    # label 파일이 없으면 None 반환 (joined만 있는 경우 무시)\n",
    "    if not os.path.exists(lp):\n",
    "        return None\n",
    "\n",
    "    # joined feature CSV 로드\n",
    "    joined_df = pd.read_csv(jp)\n",
    "    # label CSV 로드 (File, Q1, Q2만 추출)\n",
    "    label_df  = pd.read_csv(lp)\n",
    "\n",
    "    # feature DataFrame, label DataFrame 반환\n",
    "    return joined_df, label_df[[\"File\", \"Q1\", \"Q2\"]]\n"
   ],
   "id": "2f09c6f52f267693",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T10:01:25.373381Z",
     "start_time": "2025-09-30T10:01:24.309932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    save_joined_features(subject)\n",
    "    joined_df, label_df = load_subject_data(fname = subject+'.csv')\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "joined_df"
   ],
   "id": "a4f5713ea4017d0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_1_011_V2 추출한 특징 파일 병합 시작\n",
      "[Errno 2] No such file or directory: '../features/EDA/1_1_011_V2/1_1_011_V2.csv'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'joined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m      5\u001B[39m     \u001B[38;5;28mprint\u001B[39m(e)\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[43mjoined_df\u001B[49m\n",
      "\u001B[31mNameError\u001B[39m: name 'joined_df' is not defined"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "2506d674",
   "metadata": {},
   "source": [
    "## 4-2. Label 처리\n",
    "---\n",
    "### Function process_labels\n",
    "##### - Label 데이터(Q1, Q2 기반)를 처리하여 단일 Label 컬럼 생성.\n",
    "- Args:\n",
    "    - label_df (pd.DataFrame): label 데이터프레임. 반드시 \"Q1\", \"Q2\" 컬럼 포함.\n",
    "    - label_name (str): 최종 label 컬럼명 (default: \"label\").\n",
    "    - method (str): Q1/Q2 집계 방식. {\"mean\", \"min\", \"max\"} 중 선택.\n",
    "    - cut_bins (list[float] | None): label을 구간화할 구간 리스트.  \n",
    "        - 예: [-np.inf, 1, 3, np.inf] → 클래스 0/1/2.\n",
    "- Returns:\n",
    "    - pd.DataFrame: \"File\" 과 label 컬럼(label_name)만 포함된 DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "id": "dc3902c9",
   "metadata": {},
   "source": [
    "def process_labels(label_df, label_name=\"label\", method=\"mean\", cut_bins=None):\n",
    "    \"\"\"\n",
    "    label 데이터(Q1, Q2 기반)를 처리하여 단일 label 컬럼 생성.\n",
    "\n",
    "    Args:\n",
    "        label_df (pd.DataFrame): label 데이터프레임. 반드시 \"Q1\", \"Q2\" 컬럼 포함.\n",
    "        label_name (str): 최종 label 컬럼명 (default: \"label\").\n",
    "        method (str): Q1/Q2 집계 방식. {\"mean\", \"min\", \"max\"} 중 선택.\n",
    "        cut_bins (list[float] | None): label을 구간화할 구간 리스트.\n",
    "                                       예: [-np.inf, 1, 3, np.inf] → 클래스 0/1/2.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: \"File\" 과 label 컬럼(label_name)만 포함된 DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Q1, Q2 기반 라벨 계산 (평균 / 최소 / 최대)\n",
    "    if method == \"mean\":\n",
    "        label_df[label_name] = label_df[[\"Q1\", \"Q2\"]].mean(axis=1)\n",
    "    elif method == \"min\":\n",
    "        label_df[label_name] = label_df[[\"Q1\", \"Q2\"]].min(axis=1)\n",
    "    elif method == \"max\":\n",
    "        label_df[label_name] = label_df[[\"Q1\", \"Q2\"]].max(axis=1)\n",
    "    else:\n",
    "        # 지정되지 않은 method일 경우 예외 발생\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # cut_bins가 주어지면 → 연속형 값을 범주형(클래스)으로 변환\n",
    "    if cut_bins:\n",
    "        label_df[label_name] = pd.cut(\n",
    "            label_df[label_name],              # 변환할 값\n",
    "            bins=cut_bins,                     # 구간 경계값\n",
    "            labels=list(range(len(cut_bins)-1)) # 0,1,2,... 라벨링\n",
    "        ).astype(int)\n",
    "    print('Label 계산 완료')\n",
    "    # File, label_name만 반환 (다른 열은 버림)\n",
    "    return label_df[[\"File\", label_name]]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4c9ae58",
   "metadata": {},
   "source": [
    "processed_label_df = process_labels(label_df = label_df)\n",
    "processed_label_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4-3. 데이터 요약 및 통계 계산\n",
    "---\n",
    "### calculate_summary_metrics\n",
    "##### - 통합 CSV 파일을 읽어 정제 후, 키워드 기준으로 그룹화하여 통계 지표(감소율, 유의미성)를 계산.\n",
    "- Args:\n",
    "    - joint_dir (str): 분석할 데이터가 포함된 단일 CSV 파일의 전체 경로.\n",
    "- Returns:\n",
    "    - summary_df (pd.DataFrame): 계산된 모든 요약 통계 지표를 담고 있는 데이터프레임.\n",
    "        - Index: 계산된 통계 지표의 이름.\n",
    "        - intervene_val: 'intervene' 키워드를 포함하는 데이터의 평균값.\n",
    "        - craving_max: 특정 키워드('intervene', 'control', 'end')를 제외한 데이터의 최대값.\n",
    "        - decline_rate: intervene_val을 craving_max로 나눈 감소율.\n",
    "        - is_significant: decline_rate이 0.9 이하인지 여부를 나타내는 boolean 값.\n",
    "        - Columns: 데이터 정제 후 최종 피처(feature)들의 이름.\n",
    "        - Values: 각 피처에 대해 계산된 통계 지표 값.\n",
    "\n",
    "### Function analyze_and_save_subjects\n",
    "##### - 지정된 폴더의 모든 CSV 파일을 개별 분석하고, 각 결과를 별도의 CSV 파일로 저장하는 함수.\n",
    "- Args:\n",
    "    - input_folder (str): 원본 데이터 CSV 파일들이 있는 폴더 경로.\n",
    "    - output_folder (str): 결과 CSV 파일을 저장할 폴더 경로.\n",
    "    - subject_list (list): 사용할  subject의 리스트."
   ],
   "id": "46b6e04432cf5dd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_summary_metrics(joint_dir):\n",
    "    '''\n",
    "    통합 CSV 파일을 읽어 정제 후, 키워드 기준으로 그룹화하여 통계 지표(감소율, 유의미성)를 계산.\n",
    "\n",
    "    Args:\n",
    "        joint_dir (str): 분석할 데이터가 포함된 단일 CSV 파일의 전체 경로.\n",
    "\n",
    "    Returns:\n",
    "        summary_df (pd.DataFrame): 계산된 모든 요약 통계 지표를 담고 있는 데이터프레임.\n",
    "            Index: 계산된 통계 지표의 이름.\n",
    "            intervene_val: 'intervene' 키워드를 포함하는 데이터의 평균값.\n",
    "            craving_max: 특정 키워드('intervene', 'control', 'end')를 제외한 데이터의 최대값.\n",
    "            decline_rate: intervene_val을 craving_max로 나눈 감소율.\n",
    "            is_significant: decline_rate이 0.9 이하인지 여부를 나타내는 boolean 값.\n",
    "            Columns: 데이터 정제 후 최종 피처(feature)들의 이름.\n",
    "            Values: 각 피처에 대해 계산된 통계 지표 값.\n",
    "    '''\n",
    "    df = pd.read_csv(joint_dir)\n",
    "    # 데이터프레임 NaN값 들어있는 열 제거\n",
    "    df = df.dropna (axis=1)\n",
    "\n",
    "    # 데이터프레임 음수값 들어있는 열 제거\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    cols_with_negatives = [col for col in numeric_cols if (df[col] < 0).any()]\n",
    "\n",
    "    # 데이터프레임 0, inf, -inf값 들어있는 열 제거\n",
    "    values_to_remove = [0, np.inf, -np.inf]\n",
    "    cols_with_unwanted = df[numeric_cols].isin(values_to_remove).any()\n",
    "    cols_to_drop_unwanted = cols_with_unwanted[cols_with_unwanted].index\n",
    "\n",
    "    all_cols_to_drop = list(set(cols_with_negatives) | set(cols_to_drop_unwanted))\n",
    "    df_filtered = df.drop(columns=all_cols_to_drop)\n",
    "\n",
    "    feature_cols = df_filtered.select_dtypes(include=np.number).columns\n",
    "\n",
    "    # intervene1.csv 가져오기\n",
    "    intervene_mask = df_filtered['File'].str.contains('intervene', case=False, na=False)\n",
    "    intervene_means = df_filtered[intervene_mask][feature_cols].mean()\n",
    "\n",
    "    # end, control, intervene1 제외 열에서(Feature 마다) 가장 높은 값 가져오기\n",
    "    exclude_keywords = 'start|intervene|control|end'\n",
    "    exclude_mask = df_filtered['File'].str.contains(exclude_keywords, case=False, na=False)\n",
    "    craving_maxes = df_filtered[~exclude_mask][feature_cols].max()\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        'intervene_val': intervene_means,\n",
    "        'craving_max': craving_maxes\n",
    "    }).T\n",
    "\n",
    "    decline_rate = (summary_df.loc['intervene_val'] / summary_df.loc['craving_max'].replace(0, np.nan))\n",
    "    summary_df.loc['decline_rate'] = decline_rate\n",
    "    summary_df.loc['is_significant'] = summary_df.loc['decline_rate'] <= 0.9\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "def analyze_and_save_subjects(input_folder, output_folder, subject_list):\n",
    "    '''\n",
    "    지정된 폴더의 모든 CSV 파일을 개별 분석하고, 각 결과를 별도의 CSV 파일로 저장하는 함수.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): 원본 데이터 CSV 파일들이 있는 폴더 경로.\n",
    "        output_folder (str): 결과 CSV 파일을 저장할 폴더 경로.\n",
    "        subject_list (list): 사용할  subject의 리스트.\n",
    "    '''\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    csv_files = glob.glob(os.path.join(input_folder, '*.csv'))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"경고: '{input_folder}' 폴더에서 CSV 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        subject_id = os.path.basename(file_path)\n",
    "        print(f\"Analyzing: {subject_id}...\")\n",
    "        try:\n",
    "            subject_results = calculate_summary_metrics(file_path)\n",
    "            subject_df = pd.DataFrame(subject_results)\n",
    "\n",
    "            output_filename = f\"result_{subject_id}\"\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            subject_df.to_csv(output_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"'{subject_id}' 파일 처리 중 오류 발생: {e}\")\n",
    "\n",
    "    print(f\"\\n모든 분석이 완료되었습니다. 결과는 '{output_folder}' 폴더에 저장되었습니다.\")"
   ],
   "id": "83959f11985ae848",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "analyze_and_save_subjects(DATA_FOLDER, OUTPUT_FOLDER, subject_list)\n",
    "analyze_result = pd.read_csv(OUTPUT_FOLDER+'/result_'+subject+'.csv')\n",
    "analyze_result"
   ],
   "id": "16f63eab09c568b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Function combine_decline_rates\n",
    "##### - 지정된 폴더(directory) 내의 모든 CSV 파일을 읽어, 각 파일에서 'decline_rate' 행의 데이터를 추출. 데이터들을 하나의 데이터프레임으로 병합한 후, 모든 파일에 공통으로 존재하는 피처(feature)들만 남겨 최종 결과를 반환하는 함수.\n",
    "- Args\n",
    "    - directory_path (str): 요약 정보가 담긴 CSV 파일들이 저장되어 있는 폴더의 경로.\n",
    "- Returns\n",
    "    - pd.DataFrame: 아래 두 가지 경우 중 하나에 해당하는 데이터프레임을 반환.\n",
    "        - Index: 원본 CSV 파일의 이름 (예: 'result_1_01_011_V1.csv').\n",
    "        - Columns: 모든 CSV 파일에서 공통으로 발견된 피처(feature)들의 이름.\n",
    "        - Values: 각 파일의 피처별 decline_rate 값."
   ],
   "id": "dfc3c0fc92d859b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def combine_decline_rates(directory_path):\n",
    "    '''\n",
    "    지정된 폴더(directory) 내의 모든 CSV 파일을 읽어, 각 파일에서 'decline_rate' 행의 데이터를 추출. 데이터들을 하나의 데이터프레임으로 병합한 후, 모든 파일에 공통으로 존재하는 피처(feature)들만 남겨 최종 결과를 반환하는 함수.\n",
    "    Args\n",
    "        directory_path (str): 요약 정보가 담긴 CSV 파일들이 저장되어 있는 폴더의 경로.\n",
    "    Returns\n",
    "        pd.DataFrame: 아래 두 가지 경우 중 하나에 해당하는 데이터프레임을 반환.\n",
    "            Index: 원본 CSV 파일의 이름 (예: 'result_1_01_011_V1.csv').\n",
    "            Columns: 모든 CSV 파일에서 공통으로 발견된 피처(feature)들의 이름.\n",
    "            Values: 각 파일의 피처별 decline_rate 값.\n",
    "    '''\n",
    "    try:\n",
    "        csv_files = [f for f in os.listdir(directory_path) if f.endswith('.csv')]\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(\"지정된 경로에 CSV 파일이 없습니다.\")\n",
    "        print(f\"총 {len(csv_files)}개의 요약 CSV 파일을 대상으로 작업을 시작합니다...\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    all_decline_rates = []\n",
    "    for filename in csv_files:\n",
    "        filepath = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            summary_df = pd.read_csv(filepath, index_col=0)\n",
    "\n",
    "            if 'decline_rate' in summary_df.index:\n",
    "                decline_rate_series = summary_df.loc['decline_rate']\n",
    "                decline_rate_series.name = filename\n",
    "                all_decline_rates.append(decline_rate_series)\n",
    "            else:\n",
    "                print(f\"  - 경고: '{filename}' 파일에 'decline_rate' 행이 없습니다.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  - 오류: '{filename}' 파일을 처리하는 중 문제가 발생했습니다: {e}\")\n",
    "\n",
    "    if all_decline_rates:\n",
    "        final_df = pd.concat(all_decline_rates, axis=1).T\n",
    "        final_common_df = final_df.dropna(axis=1)\n",
    "\n",
    "        print(\"\\n--- 최종 결과: 공통 피쳐의 decline_rate ---\")\n",
    "        return final_common_df\n",
    "    else:\n",
    "        print(\"\\n처리할 데이터가 없습니다.\")\n",
    "        return pd.DataFrame()"
   ],
   "id": "94f8eec8b5350755",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "decline_rates_series = combine_decline_rates(OUTPUT_FOLDER).max()\n",
    "significant_features_dict = decline_rates_series[decline_rates_series <= 0.9].to_dict()\n",
    "print(len(significant_features_dict))"
   ],
   "id": "9dae7c2e0acccb73",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd85b91d",
   "metadata": {},
   "source": [
    "# 상관성 분석\n",
    "## 5-1. 상관계수 계산\n",
    "---\n",
    "### Function get_corr\n",
    "##### - 각 subject의 feature와 label 간 correlation을 계산하고, feature별 평균 correlation을 반환.\n",
    "- Args:\n",
    "    - file_list (list[str] | None): 분석할 subject 리스트. None이면 전체.\n",
    "    - cut_bins (list[float]): label 구간화 기준 (default: [-inf, 1, 3, inf]).\n",
    "    - label_name (str): label 컬럼명 (default: \"label-avg\").\n",
    "    - joined_dir (str): feature CSV 디렉토리.\n",
    "    - label_dir (str): label CSV 디렉토리.\n",
    "\n",
    "- Returns:\n",
    "    - tuple[pd.DataFrame, pd.Series]:\n",
    "            corr_df (pd.DataFrame): feature × subject correlation 테이블 + \"mean_corr\" 포함.\n",
    "\n",
    "### Function safe_corr\n",
    "##### - NaN/Inf 제거 후 샘플 수와 상수열 여부를 검사하여 Pearson correlation을 계산. 조건 미충족 시 NaN 반환.\n",
    "- Args:\n",
    "    - x (pd.Series): feature 벡터.\n",
    "    - y (pd.Series): label 벡터.\n",
    "    - min_samples (int): correlation 계산을 위한 최소 샘플 수.\n",
    "\n",
    "- Returns:\n",
    "    - float: Pearson correlation 값. 조건 미충족 시 NaN.\n",
    "\n",
    "### Function compute_correlations\n",
    "##### - feature와 label 간의 상관계수(correlation)를 계산.\n",
    "- Args:\n",
    "    - feature_df (pd.DataFrame): feature DataFrame.\n",
    "    - label_df (pd.DataFrame): label DataFrame.\n",
    "    - label_name (str): label 컬럼명 (default: \"label-avg\").\n",
    "    - min_samples (int): 상관계수 계산을 위한 최소 샘플 수.\n",
    "\n",
    "- Returns:\n",
    "    - pd.Series: 각 feature별 correlation 값 (index: feature명).  \n",
    "        - label 컬럼(label_name)은 제외됨."
   ]
  },
  {
   "cell_type": "code",
   "id": "3486b199",
   "metadata": {},
   "source": [
    "def get_corr(file_list=None, cut_bins=[-np.inf, 1, 3, np.inf], label_name=\"label\",\n",
    "             joined_dir = r\"../features/joined\", label_dir = r\"../features/label\"):\n",
    "    \"\"\"\n",
    "    각 subject의 feature와 label 간 correlation을 계산하고,\n",
    "    feature별 평균 correlation을 반환.\n",
    "\n",
    "    Args:\n",
    "        file_list (list[str] | None): 분석할 subject 리스트. None이면 전체.\n",
    "        cut_bins (list[float]): label 구간화 기준 (default: [-inf, 1, 3, inf]).\n",
    "        label_name (str): label 컬럼명 (default: \"label\").\n",
    "        joined_dir (str): feature CSV 디렉토리.\n",
    "        label_dir (str): label CSV 디렉토리.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "            corr_df: feature × subject correlation 테이블.\n",
    "                     + \"mean_corr\" 컬럼 포함.\n",
    "    \"\"\"\n",
    "    all_corr = []  # subject별 correlation 결과 저장용 리스트\n",
    "\n",
    "    # -------------------- (1) subject 파일 순회 --------------------\n",
    "    for fname in os.listdir(joined_dir):\n",
    "        if not fname.endswith(\".csv\"):   # CSV 파일만 처리\n",
    "            continue\n",
    "        subject = fname[:-4]             # 파일명에서 확장자 제거 → subject ID\n",
    "\n",
    "        # file_list가 지정되어 있으면 해당 subject만 처리\n",
    "        if file_list and subject not in file_list:\n",
    "            continue\n",
    "\n",
    "        # -------------------- (2) 데이터 로드 --------------------\n",
    "        data = load_subject_data(fname, joined_dir, label_dir)\n",
    "        if data is None:\n",
    "            continue\n",
    "        joined_df, label_df = data\n",
    "\n",
    "        # -------------------- (3) 라벨 처리 --------------------\n",
    "        # Q1, Q2 → method(\"mean\") 방식으로 통합 후,\n",
    "        # cut_bins 기준으로 클래스화\n",
    "        processed_label_df = process_labels(\n",
    "            label_df,\n",
    "            label_name=label_name,\n",
    "            method=\"mean\",\n",
    "            cut_bins=cut_bins\n",
    "        )\n",
    "\n",
    "        # -------------------- (4) feature–label correlation 계산 --------------------\n",
    "        corr_vals = compute_correlations(joined_df, processed_label_df, label_name=label_name)\n",
    "        if corr_vals is None:\n",
    "            continue\n",
    "\n",
    "        # Series에 subject 이름 붙여 저장\n",
    "        corr_vals.name = subject\n",
    "        all_corr.append(corr_vals)\n",
    "\n",
    "    # -------------------- (5) 모든 subject correlation 합치기 --------------------\n",
    "    corr_df = pd.concat(all_corr, axis=1)\n",
    "\n",
    "    # -------------------- (6) feature별 mean correlation 추가 --------------------\n",
    "    valid_counts = corr_df.count(axis=1)             # NaN 제외한 subject 수\n",
    "    mean_corr = corr_df.mean(axis=1, skipna=True)    # feature별 평균 correlation\n",
    "    mean_corr[valid_counts < 5] = np.nan             # subject 수가 적으면 NaN 처리\n",
    "    corr_df[\"mean_corr\"] = mean_corr\n",
    "\n",
    "    return corr_df\n",
    "\n",
    "def safe_corr(x, y, min_samples=10):\n",
    "    \"\"\"\n",
    "    NaN/Inf 제거 후 샘플 수와 상수열 여부를 검사하여 Pearson correlation을 계산.\n",
    "    조건 미충족 시 NaN 반환.\n",
    "\n",
    "    Args:\n",
    "        x (pd.Series): feature 벡터.\n",
    "        y (pd.Series): label 벡터.\n",
    "        min_samples (int): correlation 계산을 위한 최소 샘플 수.\n",
    "\n",
    "    Returns:\n",
    "        float: Pearson correlation 값. 조건 미충족 시 NaN.\n",
    "    \"\"\"\n",
    "    # 두 벡터를 DataFrame으로 묶음\n",
    "    df_xy = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "\n",
    "    # Inf → NaN 변환 후, NaN 값 제거\n",
    "    df_xy = df_xy.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    # 샘플 수 부족 시 NaN 반환\n",
    "    if len(df_xy) < min_samples:\n",
    "        return np.nan\n",
    "\n",
    "    # x 또는 y가 상수열(값이 모두 동일)일 경우 NaN 반환\n",
    "    if df_xy[\"x\"].nunique() <= 1 or df_xy[\"y\"].nunique() <= 1:\n",
    "        return np.nan\n",
    "\n",
    "    # Pearson correlation 계산\n",
    "    return df_xy[\"x\"].corr(df_xy[\"y\"])  # 기본값 method='pearson'\n",
    "\n",
    "\n",
    "def compute_correlations(feature_df, label_df, label_name=\"label\", min_samples=10):\n",
    "    \"\"\"\n",
    "    feature와 label(label_name) 간의 상관계수(correlation)를 계산.\n",
    "\n",
    "    Args:\n",
    "        feature_df (pd.DataFrame): feature DataFrame (각 row는 샘플, \"File\" 컬럼 포함).\n",
    "        label_df (pd.DataFrame): label DataFrame (\"File\" + label 컬럼 포함).\n",
    "        label_name (str): label 컬럼명 (default: \"label\").\n",
    "        min_samples (int): 상관계수 계산을 위한 최소 샘플 수.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: 각 feature별 correlation 값 (index: feature명).\n",
    "                   label 컬럼(label_name)은 제외됨.\n",
    "    \"\"\"\n",
    "    # \"File\" 기준으로 feature와 label 병합\n",
    "    df = pd.merge(label_df, feature_df, on=\"File\", how=\"inner\")\n",
    "    \n",
    "    # 수치형 데이터만 선택 + Inf → NaN 처리\n",
    "    numeric_df = df.select_dtypes(include=[\"number\"]).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # label_name 컬럼이 없으면 None 반환\n",
    "    if label_name not in numeric_df.columns:\n",
    "        return None\n",
    "    \n",
    "    # label 벡터 추출\n",
    "    y = numeric_df[label_name]\n",
    "    corr_vals = {}\n",
    "\n",
    "    # 각 feature별로 safe_corr 실행\n",
    "    for col in numeric_df.columns:\n",
    "        if col == label_name:\n",
    "            continue\n",
    "        corr_vals[col] = safe_corr(numeric_df[col], y, min_samples=min_samples)\n",
    "\n",
    "    # feature별 correlation 결과 반환 (Series)\n",
    "    return pd.Series(corr_vals)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e489ab00",
   "metadata": {},
   "source": [
    "corr_df = get_corr(joined_dir=DATA_FOLDER)\n",
    "corr_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_corr_sign_dict(corr_df):\n",
    "    corr_sign_dict = {\n",
    "        feature: 'pos' if value > 0 else 'neg'\n",
    "        for feature, value in corr_df.dropna().items() if value != 0\n",
    "    }\n",
    "    return corr_sign_dict"
   ],
   "id": "6eff254d7b800159",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_sign_dict = generate_corr_sign_dict(corr_df['mean_corr'])\n",
    "all_sign_dict"
   ],
   "id": "89425a9424d4fb94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_intervene(all_sign_dict, significant_features_dict):\n",
    "\n",
    "    pos_sign_dict = {\n",
    "        key: value\n",
    "        for key, value in all_sign_dict.items()\n",
    "        if value == 'pos'\n",
    "    }\n",
    "    pos_dict_label = list(pos_sign_dict.keys())\n",
    "\n",
    "    significant_features_df = pd.DataFrame(significant_features_dict, index=['intervene_Ratio']).T\n",
    "    significant_features_df['intervene_Ratio'] = 1 - significant_features_df['intervene_Ratio']\n",
    "    df_keys = significant_features_df.index.tolist()\n",
    "    valid_keys = [key for key in pos_dict_label if key in df_keys]\n",
    "    extract_df = significant_features_df.loc[valid_keys]\n",
    "    print(len(valid_keys))\n",
    "    return extract_df"
   ],
   "id": "9af0db07ccaef832",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "intervene_df = get_intervene(all_sign_dict, significant_features_dict)\n",
    "intervene_df"
   ],
   "id": "5c4aa0e1aa55fb82",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0cab837",
   "metadata": {},
   "source": [
    "## 5-2. 결과 시각화\n",
    "---\n",
    "### Function plot_feature_group\n",
    "##### - 상관계수 DataFrame(corr_df)에서 특정 feature 그룹을 추출해 heatmap으로 시각화.\n",
    "- Args:\n",
    "    - corr_df (pd.DataFrame): feature × subject correlation 테이블.\n",
    "    - keywords (list[str]): feature 이름에서 검색할 키워드 리스트.\n",
    "    - title (str): 플롯 제목.\n",
    "    - Top (int | None): 상위 N개의 feature만 표시 (None이면 전체).\n",
    "    - threshold (float): 값 강조 표시를 위한 기준치 (절댓값 기준).\n",
    "    - sort (bool): column_name 기준으로 정렬 여부.\n",
    "    - column_name (str): 표시 기준 컬럼 (\"mean_corr\"). "
   ]
  },
  {
   "cell_type": "code",
   "id": "4e390655",
   "metadata": {},
   "source": [
    "def plot_intervene_features(df, filename='feature_heatmap.png', title='Biomarker'):\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"⚠️ Warning: Input DataFrame is empty. Plotting is skipped.\")\n",
    "        return\n",
    "\n",
    "    # 1. 데이터 준비 (이전과 동일: 정렬 및 행/열 전환)\n",
    "    df_copy = df.copy()\n",
    "    df_sorted = df_copy.sort_values(by='intervene_Ratio', ascending=False)\n",
    "    df_transposed = df_sorted.T # <-- Transpose를 먼저 실행\n",
    "    df_transposed.rename(index={'intervene_Ratio': 'Intervention Ratio'}, inplace=True) # <-- Transpose된 DF의 인덱스를 변경\n",
    "\n",
    "    # 2. Figure 크기 동적 계산\n",
    "    num_features = len(df_transposed.columns)\n",
    "    fig_width = max(10, num_features * 0.8) # 최소 너비 12인치 확보\n",
    "    fig_height = 4 # 높이는 상대적으로 고정\n",
    "\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "\n",
    "    # 3. 컬러맵 설정\n",
    "    cmap = LinearSegmentedColormap.from_list(\"white_to_yellow\", [\"#ffffff\", \"#ffcc00\"])\n",
    "\n",
    "    # --- 여기가 핵심: Seaborn Heatmap 사용 ---\n",
    "    ax = sns.heatmap(\n",
    "        data=df_transposed,\n",
    "        annot=True,          # 셀 안에 숫자(값) 표시\n",
    "        fmt=\".4f\",           # 숫자를 소수점 4자리까지 표시\n",
    "        cmap=cmap,           # 위에서 정의한 커스텀 컬러맵 사용\n",
    "        # linewidths=.5,       # 셀 사이에 가는 실선 추가\n",
    "        # cbar_kws={'label': 'Intervention Ratio'} # 컬러바 레이블 설정\n",
    "    )\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # 4. 플롯 스타일링\n",
    "    ax.set_title(title, fontsize=16, weight='bold')\n",
    "\n",
    "    # x축 레이블 (피처 이름)을 45도 회전하여 가독성 확보\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # y축 레이블 ('intervene_Ratio')은 수평으로 표시\n",
    "    plt.yticks(rotation=90, weight='bold')\n",
    "\n",
    "    # 레이블이 잘리지 않도록 레이아웃 자동 조정\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 5. 이미지 저장 및 출력\n",
    "    plt.savefig(filename, dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"✅ 이미지 '{filename}' 저장 및 출력 성공!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "27e32355",
   "metadata": {},
   "source": "plot_intervene_features(intervene_df)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9a8cd76e04b4c81d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
